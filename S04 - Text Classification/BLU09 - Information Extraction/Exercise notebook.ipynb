{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-229243dce415caea",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# BLU09 - Exercises\n",
    "\n",
    "Welcome to the exercises of the BLU09! Should you get stuck on an exercise take a look at the hints or at the learning notebook in order to get some clues. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import hashlib\n",
    "import inspect\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from hashlib import sha256\n",
    "from collections import Counter\n",
    "import string\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2c9c6fdf3e87a0f7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8204f83b1061d2df",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## The Goal\n",
    "In this learning unit you are going to create a binary classifier to determine if a movie review is 'positive' or 'negative'. You will start by building some basic features, then go on to build more complex ones, and finally putting it all together. You should be able to have a working classifier by the end of the notebook. \n",
    "\n",
    "## The Dataset\n",
    "For this Exercise Notebook, you are going to use the IMDB Large movie dataset - [Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/). Each movie review has either a Positive, or a Negative label. A negative review has a score equal or less than 4 (out of 10), and a positive review has a score equal or more than 7 (out of 10). Hence, reviews with more neutral ratings are not included in the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-aa384cff83153717",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Loading Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-72fee239d1777a4a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "First of all, let's load both the train and test set into a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f7fa5bb85f101a77",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def load_imdb_sentiment_analysis_dataset(data_path, seed=42):\n",
    "\n",
    "    imdb_data_path = os.path.join(data_path, 'aclImdb')\n",
    "\n",
    "    # Load the training data\n",
    "    train_texts = []\n",
    "    train_labels = []\n",
    "    for category in ['pos', 'neg']:\n",
    "        train_path = os.path.join(imdb_data_path, 'train', category)\n",
    "        for fname in sorted(os.listdir(train_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(train_path, fname)) as f:\n",
    "                    train_texts.append(f.read())\n",
    "                train_labels.append(0 if category == 'neg' else 1)\n",
    "                \n",
    "    print(\"\\nFinished loading Train set\\n\")\n",
    "    \n",
    "    # Load the test data.\n",
    "    test_texts = []\n",
    "    test_labels = []\n",
    "    for category in ['pos', 'neg']:\n",
    "        test_path = os.path.join(imdb_data_path, 'test', category)\n",
    "        for fname in sorted(os.listdir(test_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(test_path, fname)) as f:\n",
    "                    test_texts.append(f.read())\n",
    "                test_labels.append(0 if category == 'neg' else 1)\n",
    "                \n",
    "    print(\"\\nFinished loading Test set\\n\")\n",
    "    \n",
    "    # Shuffle the training data and labels.\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_texts)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_labels)\n",
    "    \n",
    "    return ((train_texts, np.array(train_labels)),\n",
    "            (test_texts, np.array(test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warnings: The two cells below might take a few minutes depending on your machine..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5d2fb6dc78356253",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = load_imdb_sentiment_analysis_dataset(\"datasets/\")\n",
    "df = pd.DataFrame(data=[x_train, y_train], index=[\"text\", \"label\"]).T\n",
    "df = df.append(pd.DataFrame(data=[x_test, y_test], index=[\"text\", \"label\"]).T)[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bbc68a16f01c06dc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# load the small-sized SpaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "en_stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "\n",
    "# Create a list of SpaCy \"Docs\" by leveraging the SpaCy pipeline\n",
    "docs = list(nlp.pipe(df.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-58afbb4ebfcde68b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's have a look at the first 2 reviews to understand the text we are dealing with ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0ce84919fe5c0179",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "docs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-76826c54dbe7c129",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q1 - Text cleaning\n",
    "\n",
    "Looking at the text above, you see that there are several HTML tags. First, let's clean 'em up! BeautifulSoup has a cool `get_text()` method that strips all the leftover html tags. Then let's use Regex, something that you have learned previously, to remove all the punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e6a921c3380d7349",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text)\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_punct(text):\n",
    "    #remove everything except words, digits and space\n",
    "    text = re.sub(r'[^\\w\\s]','',text) \n",
    "        \n",
    "    #regex often miss the underscore so let's remove that as well\n",
    "    text = re.sub(r'\\_','',text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text, stopwords):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [tok.lower() for tok in tokens]\n",
    "    if stopwords:\n",
    "        tokens = [tok for tok in tokens if tok not in stopwords]\n",
    "\n",
    "    text_processed = ' '.join(tokens)\n",
    "    return text_processed\n",
    "\n",
    "def preprocessing(df):\n",
    "    \"\"\"\n",
    "    Implement the three above functions in the respective order to remove html tags, punctuations and stopwords\n",
    "    Hint: Use the apply function.\n",
    "    \n",
    "    \"\"\"\n",
    "    df_ = df.copy()\n",
    "    \n",
    "    #df_['text'] = df_['text'].apply(...).apply(...).apply(...)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4e472878224d73d2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's clean, and process the df\n",
    "df_raw = df.copy()\n",
    "df = preprocessing(df)\n",
    "value_hash = '81596d9ecc63f0a3d1b634903b64affc939a27cb09ebe297d4c0d9697ca2bb11'\n",
    "assert sha256(str(df['text']).encode()).hexdigest() == value_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3964f5278d3e2e45",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q2 - Text exploration with SpaCy \n",
    "\n",
    "Now that we have cleaned the data, let's start extracting some useful features. We will first start simple and perform some exploration using `SpaCy`.\n",
    "\n",
    "### Q2.a) Create a simple matcher\n",
    "You suspect that some positive words such as \"excellent\", \"classic\", and \"great\" often occur in Positive reviews. Let's quickly test that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a56287fb46358b4e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "for word in ['classic', 'excellent', 'great']:\n",
    "    print(word)\n",
    "    print(df[df['text'].str.contains(word)].label.value_counts())\n",
    "    print('-------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-afd00f2b839c84f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Indeed, your intuition is right. It's clear that those positive words are more likely to occur in Positive reviews. \n",
    "\n",
    "Now, take advantage of SpaCy's `Matcher` to count the total *exact* number of matches of these words. Looking at the below figure should help you choose the pattern to use for this purpose.\n",
    "\n",
    "![](media/token_attributes.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e1c582f5472593cf",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Count the number of total exact matches of the words \"excellent\", \"great\", and \"classic\" using the SpaCy Matcher and assign it to \"count\"\n",
    "#matcher = Matcher(...)\n",
    "#\n",
    "#for ... :\n",
    "#    pattern = [...]\n",
    "#    matcher.add(...)\n",
    "#\n",
    "#count =0\n",
    "#for doc in docs:\n",
    "#    matches = ...\n",
    "#    count += ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8c4c03bc54acfdc0",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "count_hash = '33e14c27247dae6ca2ac565cf7d5fa4200defa487918c52a2dfcccb6d09b4329'\n",
    "assert sha256(str(count).encode()).hexdigest() == count_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-90610064b252d533",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Q2.b) Extract Emojis\n",
    "\n",
    "Looking at a few review examples, you realized that people tend to use emojis in their reviews. Perhaps we could extract some signals out of these? \n",
    "\n",
    "Let's build a matcher to extract positive emojis & negative emojis from the text and store their counts in `positive_emojis_count` and `negative_emojis_count`. \n",
    "\n",
    "You can easily do this with Regex - Spacy allows us to add the `REGEX` operator to our Matcher object. Hint: Check out [Spacy's documentation](https://spacy.io/usage/rule-based-matching#regex) to learn how to do that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cdebc756314d71d9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "nlp = English()  # We only want the tokenizer, so no need to load a model\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "#pos_patterns = ['TEXT': ....] - For Positive emoji let's use \":)\"\n",
    "#neg_patterns = ['TEXT': ....] - For Negative emoji let's use \":(\"\n",
    "\n",
    "# Hint - Don't forget to escape the special character \"(\" and \")\"\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "def count_emoji_matches(pattern, docs = docs):\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"EMOJIS\", None, pattern)\n",
    "    \n",
    "    n_emojis = []\n",
    "    for doc in docs:\n",
    "        matches = matcher(doc)\n",
    "        emojis_count = len(matches)\n",
    "        for match in matches:\n",
    "            emojis_count += 1\n",
    "        n_emojis.append(emojis_count)\n",
    "            \n",
    "    return n_emojis\n",
    "\n",
    "positive_emojis_count = sum(count_emoji_matches(pos_patterns))\n",
    "negative_emojis_count = sum(count_emoji_matches(neg_patterns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-790c430e04cb0f1e",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "positive_hash = '7688b6ef52555962d008fff894223582c484517cea7da49ee67800adc7fc8866'\n",
    "negative_hash = 'd4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35'\n",
    "assert sha256(str(positive_emojis_count).encode()).hexdigest() == positive_hash\n",
    "assert sha256(str(negative_emojis_count).encode()).hexdigest() == negative_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-880531b38c9b67b6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Q2.c) Extract Part of Speech features\n",
    "\n",
    "You also think that negative reviews may have several adverbs followed by an adjective to express the extent to which how bad a movie is (e.g. ridiculously bad, unbelievable awful)\n",
    "\n",
    "To help you, here's the list of PoS available in SpaCy:\n",
    "\n",
    "![](media/pos_helper.png)\n",
    "\n",
    "To complete this exercise you should build a matcher to extract all adverbs that are followed by an adjective. Store this sequence in a list, and assign the result to `adv_adj_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-64683287d0596aeb",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#Store all the adv-adj sequence in a list called adv_adj_list\n",
    "#matcher = ...\n",
    "#pattern = [...]\n",
    "#matcher.add(...)\n",
    "#\n",
    "#adv_adj_list = []\n",
    "#for doc in docs:\n",
    "#    matches = ...\n",
    "#    for ... in matches:\n",
    "#        adv_adj_list.append(...)\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2d0d7a2d2ed5ed85",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "list_hash = '1cde2b9de1483573a1d70aed1fc8f90eb8c3f18a992f289543e3aa4c09a14edd'\n",
    "assert len(adv_adj_list) == 15679\n",
    "assert sha256(','.join(map(str, adv_adj_list)).encode()).hexdigest() == list_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e178d0f93c0b875c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q2.d) Extract entities\n",
    "\n",
    "Your intuition is that Positive Reviews are likely to describe the movie plot, citing several locations in the movie. An idea is to extract some locations from the text.\n",
    "\n",
    "Build a `Matcher` to match Location in the text and extract the top 10 most common ones. Assign them to `most_common_ents`.\n",
    "\n",
    "*hint: Use [Counter](https://docs.python.org/3/library/collections.html#collections.Counter) to extract the most common elements (check the most_common(n) method). You will need to feed it strings (not SpaCy spans)*\n",
    "\n",
    "*note: in a real-case scenario we would perform some text preprocessing first and build a better entity recognizer, but let us not worry about that here*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7c82f29c4da571fd",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Build a matcher to extract the location-type entities from the text and assign them to most_common_ents\n",
    "#\n",
    "#matcher = ...\n",
    "#\n",
    "#pattern = [...]\n",
    "#matcher.add(...)\n",
    "# \n",
    "#...\n",
    "#\n",
    "# most_common_ents = ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f7cb2acd87b1549d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "ent_hash = \"35eb8667c8e28dacaba6290382dc6d46e4df777b27d1eef8678e099ad359de25\"\n",
    "assert len(most_common_ents) == 10\n",
    "assert sha256(','.join(map(str, most_common_ents)).encode()).hexdigest() == ent_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fb8248e18ff20bd6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that we have the most common locations, let's quickly check its usefulness and whether we should include it as a feature. \n",
    "\n",
    "Indeed, as can be seen below, locations are more likely to occur in Positive Reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-64f5f34fb17c25bc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "most_common_locations = [loc[0] for loc in most_common_ents]\n",
    "\n",
    "for word in most_common_locations:\n",
    "    print(word)\n",
    "    print(df_raw[df_raw['text'].str.contains(word)].label.value_counts())\n",
    "    print('-------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-45e34f159eafa6a7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q3 - Create Numerical Features\n",
    "\n",
    "You start thinking what features could actually be useful for solving your problem. One possible factor that may help is to know the number of adjectives used, the length of the review, average word length and the count of positive & negative emojis\n",
    "\n",
    "Add extra fields to the `df` dataframe with:\n",
    "- The count of the number of adjectives - consider the adjectives as those identified by SpaCy\n",
    "- The length of the document - you can simply count the number of characters. \n",
    "- The average word length - you learned how to do this in Learning Notebook - you don't need to remove stopwords as we already did it in the beginning\n",
    "- The count of positive emojis - Hint: use `count_emoji_matches` function in Q2b.\n",
    "- The count of negative emojis - Hint: use `count_emoji_matches` function in Q2b.\n",
    "\n",
    "Assign the number of adjectives to a new column called `n_adjs`, the length of the reviews to a column called `text_length`, average word length to a column called `avg_word_length`, and the count of positive and negative emojis to two columns called `positive_emojis_count` and `negative_emojis_count`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bab0e5a3f8f8cbc7",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Hint: you can iterate over the tokens in Spacy doc to inspect them \n",
    "# for doc in docs:\n",
    "#    print(doc.ents)\n",
    "#    for token in doc:\n",
    "#        print(token.pos_)\n",
    "\n",
    "#n_adjs = []\n",
    "#\n",
    "#for doc in docs:\n",
    "#    count_adjs = 0\n",
    "#    ...\n",
    "#    ...\n",
    "#    n_adjs.append(count_adjs)\n",
    "#\n",
    "#df['n_adjs'] = n_adjs\n",
    "#df['text_length'] = ...\n",
    "#df['avg_word_length'] = ...\n",
    "#df['positive_emojis_count'] = count_emoji_matches(...)\n",
    "#df['negative_emojis_count'] = count_emoji_matches(...)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-03dbc9fa2ea23663",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert all(col in df.columns for col in ('n_adjs', 'avg_word_length', 'text_length', 'positive_emojis_count', 'negative_emojis_count'))\n",
    "assert df.n_adjs.sum() == 101733\n",
    "assert np.allclose(df.avg_word_length.sum(), 29805, 5)\n",
    "assert df.text_length.sum() == 3829351\n",
    "assert df.positive_emojis_count.sum() == 56\n",
    "assert df.negative_emojis_count.sum() == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-923d795a48aa24c6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q4 - Pipelines and Feature Unions\n",
    "It is now time for you to leverage on your newly built features and construct pipelines that can be fed to a classifier. You decide to use a [Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) as you hear from industry experts it tends to work well for text classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9c8ff5d97624ac9b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_data.label = train_data.label.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7084694ab285eb01",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class Selector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a column from the dataframe to perform additional transformations on\n",
    "    \"\"\" \n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "\n",
    "class TextSelector(Selector):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on text columns in the data\n",
    "    \"\"\"\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "    \n",
    "    \n",
    "class NumberSelector(Selector):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]\n",
    "\n",
    "    \n",
    "def get_accuracy(feats, train_data, test_data):\n",
    "    \"\"\"\n",
    "    Return the accuracy on the test_data by using a RandomForestClassifier trained on the \n",
    "    train_data with the features described by feats\n",
    "    \"\"\"\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('features',feats),\n",
    "        ('classifier', RandomForestClassifier(random_state = 42, n_estimators=10)),\n",
    "    ])\n",
    "\n",
    "    pipeline.fit(train_data, train_data.label)\n",
    "\n",
    "    preds = pipeline.predict(test_data)\n",
    "    accuracy = np.mean(preds == test_data.label)\n",
    "    \n",
    "    print(\"Accuracy: {:.4f}\".format(accuracy))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-42ba44a8653efe95",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Q4.a) Build a Feature Union\n",
    "You hypothesize that combining the text and numerical features could help you build a strong classifier. \n",
    "\n",
    "Use `FeatureUnion` to join:\n",
    "- The Text features extracted from a standard sklearn `TfidfVectorizer` (with $ngram\\_range=(1,2)$)\n",
    "- The numeric feature of the length of the messages scaled to zero mean and unit variance *[hint](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)*\n",
    "- The average word length that you have created previously\n",
    "\n",
    "Assign the Feature Union to a variable named `feats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0848e748dc61721f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#text_pipe = Pipeline(...)\n",
    "#text_len_pipe =  Pipeline(...)\n",
    "#word_len_pipe =  Pipeline(...)\n",
    "#feats = FeatureUnion(...)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-80971dd7f21ec905",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(feats, FeatureUnion)\n",
    "assert any(isinstance(obj, Selector) for obj in feats.transformer_list[0][1])\n",
    "assert any(isinstance(obj, TfidfVectorizer) for obj in feats.transformer_list[0][1])\n",
    "assert np.allclose(get_accuracy(feats, train_data, test_data), 0.7290, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1f4ecd3a08b31cb9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Q4.b) Add more features\n",
    "You decide to try adding the number of adjectives to your features to see if they can improve the performance of your classifier. \n",
    "\n",
    "On top of all features you have used for `feats`, add the number of adjectives `n_adjs` that you computed in Q3 to your features. Then assign your features to `feats_v2`. There should be 4 features in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0ef7216ae6023fdf",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#adjs_pipe = Pipeline(...)\n",
    "#...\n",
    "#feats_v2 = FeatureUnion(...)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2726b142fbec4f14",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "accuracy = get_accuracy(feats_v2, train_data, test_data)\n",
    "assert np.allclose(accuracy, 0.6860, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b1aa91b08408579d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Q4.c) Add the Emojis feature\n",
    "You try to improve your model even further by including the number of emojis `positive_emojis_count` and `negative_emojis_count` features that you created above. \n",
    "\n",
    "On top of all features in `feats_v2`, add the number of emojis to your features and assign the result to `feats_v3` (**no need to scale** the features this time). There should be 6 features in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f5770ba6afa04742",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#...\n",
    "#feats_v3 = FeatureUnion(...)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8289104bef520ea7",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "accuracy = get_accuracy(feats_v3, train_data, test_data)\n",
    "assert np.allclose(accuracy, 0.7010, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dcbab21d6897ff51",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You realize that your accuracy actually decreased, which reminds you that more features does not necessarily mean better results.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "You realize you can get fairly ok accuracy on the sentiment analysis problem using a fairly simple solution. You know there are many things you could improve (e.g. Dimensionality Reduction) and many further paths you could choose in order to try to take your classifier to the next level, but you decide to leave that challenge for another day. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
