{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "\n",
    "from collections import Counter, OrderedDict\n",
    "from sklearn.base import TransformerMixin,BaseEstimator\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import math\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import string\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.svm import SVC\n",
    "import spacy\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from spacy.lang.en import English\n",
    "\n",
    "from pylab import barh,plot,yticks,show,grid,xlabel,figure,cla,close\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". - matches any character, except newline.\n",
    "\n",
    "\\d, \\s \\S - match digit, match whitespace, not whitespace.\n",
    "\n",
    "\\b, \\B - word, not word boundary.\n",
    "\n",
    "[xyz] - matches x, y or z.\n",
    "\n",
    "[^xyz] - matches anything that is not x, y or z.\n",
    "\n",
    "[x-z] - matches a character between x and z.\n",
    "\n",
    "^xyz$ - ^ is the start of the string, $ is the end of the string.\n",
    "\n",
    "\\. - use escaping to match special characters.\n",
    "\n",
    "\\t, \\n - matches tab and newline.\n",
    "\n",
    "x* - matches 0 or more symbols x.\n",
    "\n",
    "x+ - matches 1 or more symbols x.\n",
    "\n",
    "x? - matches 0 or 1 symbol x.\n",
    "\n",
    ".?, *?, +?, etc - represent non-greedy search.\n",
    "\n",
    "x{5} - matches exactly 5 symbols x.\n",
    "\n",
    "x{5,} - matches 5 or more symbols x.\n",
    "\n",
    "x{5, 8} - matches between 5 and 8 symbols x.\n",
    "\n",
    "xy|yz - matches xy or yz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "#stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "#stems = [list(map(stemmer.stem, words))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>American researcher Charles Lieber was arrest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Did Trump Blame Obama for ‘Bad’ COVID-19 Tests...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hundreds of sampoerna cigarette factory worke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dr. Megha Vyas from Pune died of COVID-19.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jacob Rothschild owns a patent to coronavirus.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0   American researcher Charles Lieber was arrest...\n",
       "1  Did Trump Blame Obama for ‘Bad’ COVID-19 Tests...\n",
       "2   Hundreds of sampoerna cigarette factory worke...\n",
       "3         Dr. Megha Vyas from Pune died of COVID-19.\n",
       "4     Jacob Rothschild owns a patent to coronavirus."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the dataset\n",
    "##there are two columns in the data set sentiment which is positive and negative and text columns this is movie review\n",
    "#df = pd.read_csv('./datasets/twitter.csv')\n",
    "df_train_nb = pd.read_csv('./data/covid19_data.csv')#, encoding='latin1')\n",
    "#df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1,inplace=True)\n",
    "df_train_nb = df_train_nb[['class','title']]\n",
    "df_train_nb.rename(columns={\"class\":\"target\", \"title\":\"text\"},inplace=True)\n",
    "df_test= pd.read_csv('data/covid19_unlabelled_test.csv')\n",
    "\n",
    "df_test = df_test[['title']]\n",
    "df_test.rename(columns={\"title\":\"text\"},inplace=True)\n",
    "df_test.head()\n",
    "#NOTE: If the data set is so many choose part of that in order to accelerate the process of analysing\n",
    "    ##docs = df.text[:200]\n",
    "# Get the text\n",
    "#docs = df['text']\n",
    "\n",
    "\n",
    "\n",
    "###we can read from file and represent it as list as follows\n",
    "#def file_to_list(file_name):\n",
    "    #with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        #return [line.strip() for line in f.readlines()]\n",
    "    \n",
    "#X_train_pre = file_to_list('data/tweets_train_preprocessed.txt')\n",
    "#X_dev_pre = file_to_list('data/tweets_dev_preprocessed.txt')\n",
    "#X_test_pre = file_to_list('data/tweets_test_preprocessed.txt')\n",
    "\n",
    "##############################################################Read a JSON file################################\n",
    "\n",
    "#docs = []\n",
    "#with open('./datasets/sample_data.json') as fp:\n",
    "    #for line in fp:\n",
    "        #entry = json.loads(line)\n",
    "        #docs.append(entry['body'])\n",
    "        \n",
    "#print('I read {} documents'.format(len(docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['FALSE', 'False', 'Correct Attribution', 'True', 'Explanatory',\n",
       "       'false', 'Misleading', 'MISLEADING', 'Mostly True', 'No Evidence',\n",
       "       'partly false', 'Mostly false', 'Partly false', 'PARTLY FALSE',\n",
       "       'No evidence', 'Mostly False', 'misleading', 'Labeled Satire',\n",
       "       'News', 'Partly False', 'Misleading/False', 'Mixture',\n",
       "       'no evidence', 'MOSTLY FALSE', 'Misattributed', 'Unproven',\n",
       "       'Miscaptioned', 'Fake', 'Half True', 'PARTLY TRUE', 'Not true',\n",
       "       'Scam', \"(Org. doesn't apply rating)\", 'Partially false',\n",
       "       'MOSTLY TRUE', 'Partly true', 'mislEADING', 'NO EVIDENCE',\n",
       "       'half true', 'false and misleading', 'mostly false', nan,\n",
       "       'HALF TRUE', 'Two Pinocchios', 'Suspicions', 'Partly FALSE',\n",
       "       'PANTS ON FIRE', 'Correct', 'Misinformation / Conspiracy theory',\n",
       "       'IN DISPUTE', 'HALF TRUTH', 'MiSLEADING', 'Partially correct',\n",
       "       'Unlikely', 'Fake news', 'True but', 'Mostly true', 'Collections',\n",
       "       'Mixed', 'Unverified', 'Partially true'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_nb['target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A viral image suggests that cocaine kills cor...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Having sex every morning kills coronavirus.</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Picture shows the Mecca totally empty.</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Queen Elizabeth is shown in a quote card to b...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Official news: in Naples people have used Toc...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1755</th>\n",
       "      <td>Bill Gates patented the coronavirus.</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1756</th>\n",
       "      <td>Vitamin D Supplements prevent health problems...</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1757</th>\n",
       "      <td>Smoking may protect against COVID-19; scienti...</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1758</th>\n",
       "      <td>Ibuprofen enhances the coronavirus.</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1759</th>\n",
       "      <td>Did genetic mutations cause the coronavirus t...</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1760 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text target\n",
       "0      A viral image suggests that cocaine kills cor...      f\n",
       "1           Having sex every morning kills coronavirus.      f\n",
       "2                Picture shows the Mecca totally empty.      f\n",
       "3      Queen Elizabeth is shown in a quote card to b...      f\n",
       "4      Official news: in Naples people have used Toc...      f\n",
       "...                                                 ...    ...\n",
       "1755               Bill Gates patented the coronavirus.      t\n",
       "1756   Vitamin D Supplements prevent health problems...      t\n",
       "1757   Smoking may protect against COVID-19; scienti...      t\n",
       "1758                Ibuprofen enhances the coronavirus.      t\n",
       "1759   Did genetic mutations cause the coronavirus t...      t\n",
       "\n",
       "[1760 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('./data/df_10percTrue_with_noevidence.csv')\n",
    "df_train.drop([\"Unnamed: 0\"], axis=1,inplace=True)\n",
    "df_train.rename(columns={\"label\":\"target\", \"title\":\"text\"},inplace=True)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###function define to transform target column to true and false\n",
    "def clean_targ_col(df_):\n",
    "    df_['target'] = df_['target'].replace({'False' : 'false',\n",
    "    'FALSE' : 'false',\n",
    "    'Misleading' : 'false',\n",
    "    'MISLEADING' : 'false',\n",
    "    'Mostly false' : 'false',\n",
    "    'Partly false': 'false',\n",
    "    'misleading' : 'false',\n",
    "    'No evidence' : 'true',\n",
    "    'Mostly False' : 'false',\n",
    "    'Mixture': 'false',\n",
    "    'True': 'true',\n",
    "    'Explanatory': 'true',\n",
    "    'No Evidence': 'true',\n",
    "    'News': 'true',\n",
    "    'PARTLY FALSE': 'false',\n",
    "    'Unproven': 'false',\n",
    "    'MOSTLY FALSE': 'false',\n",
    "    'Partly False': 'false',\n",
    "    'Miscaptioned': 'false',\n",
    "    'partly false': 'false',\n",
    "    'mostly false' : 'false',\n",
    "    'Mostly True': 'true',\n",
    "    'MOSTLY TRUE': 'true',\n",
    "    'Misattributed': 'false',\n",
    "    \"(Org. doesn't apply rating)\": 'false',\n",
    "    'HALF TRUE': 'true',\n",
    "    'Correct Attribution' :'true',\n",
    "    'Partially false': 'false',\n",
    "    'Labeled Satire': 'false',\n",
    "    'Fake' : 'false',\n",
    "    'NO EVIDENCE': 'true',\n",
    "    'false' : 'false',\n",
    "    'Two Pinocchios': 'false',\n",
    "    'Scam' : 'false',\n",
    "    'no evidence': 'true',\n",
    "    'Half True': 'true',\n",
    "    'PARTLY TRUE': 'true',\n",
    "    'half true': 'true',\n",
    "    'Correct': 'true',\n",
    "    'mislEADING' : 'false',\n",
    "    'Suspicions': 'false',\n",
    "    'Not true' : 'false',\n",
    "    'nan': 'false',\n",
    "    'Partly FALSE': 'false',\n",
    "    'Misleading/False' : 'false',\n",
    "    'PANTS ON FIRE': 'false',\n",
    "    'Partially true': 'true',\n",
    "    'Mixed': 'false',\n",
    "    'IN DISPUTE': 'false',\n",
    "    'Unverified': 'false',\n",
    "    'HALF TRUTH': 'true',\n",
    "    'Collections': 'false',\n",
    "    'Partially correct': 'true',\n",
    "    'MiSLEADING' : 'false',\n",
    "    'Mostly true': 'true',\n",
    "    'True but': 'true',\n",
    "    'false and misleading' : 'false',\n",
    "    'Partly true': 'true',\n",
    "    'Misinformation / Conspiracy theory' : 'false',\n",
    "    'Unlikely' : 'false',\n",
    "    'Fake news' : 'false'})\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A viral image suggests that cocaine kills cor...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Having sex every morning kills coronavirus.</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Picture shows the Mecca totally empty.</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Queen Elizabeth is shown in a quote card to b...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Official news: in Naples people have used Toc...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1755</th>\n",
       "      <td>Bill Gates patented the coronavirus.</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1756</th>\n",
       "      <td>Vitamin D Supplements prevent health problems...</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1757</th>\n",
       "      <td>Smoking may protect against COVID-19; scienti...</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1758</th>\n",
       "      <td>Ibuprofen enhances the coronavirus.</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1759</th>\n",
       "      <td>Did genetic mutations cause the coronavirus t...</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1760 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text target\n",
       "0      A viral image suggests that cocaine kills cor...      f\n",
       "1           Having sex every morning kills coronavirus.      f\n",
       "2                Picture shows the Mecca totally empty.      f\n",
       "3      Queen Elizabeth is shown in a quote card to b...      f\n",
       "4      Official news: in Naples people have used Toc...      f\n",
       "...                                                 ...    ...\n",
       "1755               Bill Gates patented the coronavirus.      t\n",
       "1756   Vitamin D Supplements prevent health problems...      t\n",
       "1757   Smoking may protect against COVID-19; scienti...      t\n",
       "1758                Ibuprofen enhances the coronavirus.      t\n",
       "1759   Did genetic mutations cause the coronavirus t...      t\n",
       "\n",
       "[1760 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_clean = clean_targ_col(df_train)\n",
    "df_train_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding the target coulmns to 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels\n",
    "#le = preprocessing.LabelEncoder()\n",
    "#le.fit(df_train_clean['target'].values)\n",
    "#df_train_clean['target'] = le.transform(df_train_clean['target'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A viral image suggests that cocaine kills cor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Having sex every morning kills coronavirus.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Picture shows the Mecca totally empty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Queen Elizabeth is shown in a quote card to b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Official news: in Naples people have used Toc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1755</th>\n",
       "      <td>Bill Gates patented the coronavirus.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1756</th>\n",
       "      <td>Vitamin D Supplements prevent health problems...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1757</th>\n",
       "      <td>Smoking may protect against COVID-19; scienti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1758</th>\n",
       "      <td>Ibuprofen enhances the coronavirus.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1759</th>\n",
       "      <td>Did genetic mutations cause the coronavirus t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1760 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target\n",
       "0      A viral image suggests that cocaine kills cor...       0\n",
       "1           Having sex every morning kills coronavirus.       0\n",
       "2                Picture shows the Mecca totally empty.       0\n",
       "3      Queen Elizabeth is shown in a quote card to b...       0\n",
       "4      Official news: in Naples people have used Toc...       0\n",
       "...                                                 ...     ...\n",
       "1755               Bill Gates patented the coronavirus.       1\n",
       "1756   Vitamin D Supplements prevent health problems...       1\n",
       "1757   Smoking may protect against COVID-19; scienti...       1\n",
       "1758                Ibuprofen enhances the coronavirus.       1\n",
       "1759   Did genetic mutations cause the coronavirus t...       1\n",
       "\n",
       "[1760 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_clean['target'] = df_train_clean['target'].replace({'f':0,'t':1})\n",
    "df_train_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nc_train = df_train_clean.dropna().copy()\n",
    "df_nc_test = df_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split raw data without cleaning to train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in train and validation\n",
    "train_nc_df, validation_nc_df = train_test_split(df_nc_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_nc_df.target.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer to implement sentence cleaning\n",
    "class TextCleanerTransformer(TransformerMixin):\n",
    "    def __init__(self, tokenizer, stemmer, regex_list,\n",
    "                 lower=True, remove_punct=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stemmer = stemmer\n",
    "        self.regex_list = regex_list\n",
    "        self.lower = lower\n",
    "        self.remove_punct = remove_punct\n",
    "        \n",
    "    def transform(self, X, *_):\n",
    "        X = list(map(self._clean_sentence, X))\n",
    "        return X\n",
    "    \n",
    "    def _clean_sentence(self, sentence):\n",
    "        \n",
    "        # Replace given regexes\n",
    "        for regex in self.regex_list:\n",
    "            sentence = re.sub(regex[0],regex[1], sentence)\n",
    "            \n",
    "        # lowercase\n",
    "        if self.lower:\n",
    "            sentence = sentence.lower()\n",
    "\n",
    "        # Split sentence into list of words\n",
    "        words = self.tokenizer.tokenize(sentence)\n",
    "            \n",
    "        # Remove punctuation\n",
    "        if self.remove_punct:\n",
    "            words = list(filter(lambda x: x not in string.punctuation, words))\n",
    "\n",
    "        # Stem words\n",
    "        if self.stemmer:\n",
    "            words = map(self.stemmer.stem, words)\n",
    "\n",
    "        # Join list elements into string\n",
    "        sentence = \" \".join(words)\n",
    "        \n",
    "        return sentence\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data by function not class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text)\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_punct(text):\n",
    "    #remove everything except words, digits and space\n",
    "    text = re.sub(r'[^\\w\\s]','',text) \n",
    "        \n",
    "    #regex often miss the underscore so let's remove that as well\n",
    "    text = re.sub(r'\\_','',text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text, stopwords):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [tok.lower() for tok in tokens]\n",
    "    if stopwords:\n",
    "        tokens = [tok for tok in tokens if tok not in stopwords]\n",
    "    tokens = [stemmer.stem(tok) for tok in tokens]\n",
    "\n",
    "    text_processed = ' '.join(tokens)\n",
    "    return text_processed\n",
    "\n",
    "\n",
    "\n",
    "def preprocessing_(df):\n",
    "    \"\"\"\n",
    "    Implement the three above functions in the respective order to remove html tags, punctuations and stopwords\n",
    "    Hint: Use the apply function.\n",
    "    \n",
    "    \"\"\"\n",
    "    df_ = df.copy()\n",
    "    \n",
    "    #df_['text'] = df_['text'].apply(...).apply(...).apply(...)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    df_['text'] = df_['text'].apply(remove_html_tags).apply(remove_punct).apply(remove_stopwords,stopwords=en_stopwords)\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a tokenizer and a stemmer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "#regex_list = [(\"<[^>]*>\", \"\")]\n",
    "regex_list = [(\"\\#\",\"\")]\n",
    "\n",
    "#cleaner = TextCleanerTransformer(tokenizer, stemmer, regex_list)\n",
    "#docs = cleaner.transform(train_df.text.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define pipline and predict MultinomialNB()(Did not work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build the pipeline\n",
    "#text_clf = Pipeline([('prep', TextCleanerTransformer(tokenizer, stemmer,regex_list)),\n",
    " #                  ('vect', CountVectorizer(stop_words='english', ngram_range=(1,2))),\n",
    "  #                 ('tfidf', TfidfTransformer()),\n",
    "   #                ('clf', MultinomialNB())])\n",
    "# Train the classifier\n",
    "##text_clf.fit(map(str, train_df['text'].values), train_df['sentiment'].values)\n",
    "#text_clf.fit(map(str, train_nc_df['text'].values), train_nc_df['target'].values)\n",
    "\n",
    "##predicted = text_clf.predict(map(str, validation_df['text'].values))\n",
    "##np.mean(predicted == validation_df['sentiment'])\n",
    "\n",
    "#predicted_val = text_clf.predict(map(str, validation_nc_df['text'].values))\n",
    "#np.mean(predicted_val == validation_nc_df['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define pipeline and predict RandomForrestcalssifier(I am not using this right now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9034090909090909"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the pipeline\n",
    "text_clf = Pipeline([('prep', TextCleanerTransformer(tokenizer, stemmer, regex_list)),\n",
    "                   ('vect', CountVectorizer(stop_words='english', ngram_range=(1,2))),\n",
    "                   ('tfidf', TfidfTransformer()),\n",
    "                   ('classifier', RandomForestClassifier(random_state = 42))])\n",
    "# Train the classifier\n",
    "##text_clf.fit(map(str, train_df['text'].values), train_df['sentiment'].values)\n",
    "text_clf.fit(map(str, train_nc_df['text'].values), train_nc_df['target'].values)\n",
    "\n",
    "##predicted = text_clf.predict(map(str, validation_df['text'].values))\n",
    "##np.mean(predicted == validation_df['sentiment'])\n",
    "\n",
    "predicted_val = text_clf.predict(map(str, validation_nc_df['text'].values))\n",
    "np.mean(predicted_val == validation_nc_df['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check classification result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.99      0.95       315\n",
      "           1       0.71      0.14      0.23        37\n",
      "\n",
      "    accuracy                           0.90       352\n",
      "   macro avg       0.81      0.56      0.59       352\n",
      "weighted avg       0.89      0.90      0.87       352\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the results\n",
    "##print(classification_report(y_dev, y_dev_pred))\n",
    "print(classification_report(validation_nc_df['target'].values,predicted_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test = text_clf.predict(map(str, df_nc_test['text'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>American researcher Charles Lieber was arrest...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Did Trump Blame Obama for ‘Bad’ COVID-19 Tests...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hundreds of sampoerna cigarette factory worke...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dr. Megha Vyas from Pune died of COVID-19.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jacob Rothschild owns a patent to coronavirus.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2087</th>\n",
       "      <td>President Emmanuel Macron was cheered by a cr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088</th>\n",
       "      <td>Five helicopters are going to spray the air w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2089</th>\n",
       "      <td>Coronavirus was created in a lab in order to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2090</th>\n",
       "      <td>Video of a man dressed in white who recommend...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2091</th>\n",
       "      <td>President Donald Trump implemented “a travel ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2092 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  predicted\n",
       "0      American researcher Charles Lieber was arrest...          0\n",
       "1     Did Trump Blame Obama for ‘Bad’ COVID-19 Tests...          1\n",
       "2      Hundreds of sampoerna cigarette factory worke...          0\n",
       "3            Dr. Megha Vyas from Pune died of COVID-19.          0\n",
       "4        Jacob Rothschild owns a patent to coronavirus.          0\n",
       "...                                                 ...        ...\n",
       "2087   President Emmanuel Macron was cheered by a cr...          0\n",
       "2088   Five helicopters are going to spray the air w...          0\n",
       "2089   Coronavirus was created in a lab in order to ...          0\n",
       "2090   Video of a man dressed in white who recommend...          0\n",
       "2091   President Donald Trump implemented “a travel ...          0\n",
       "\n",
       "[2092 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nc_test['predicted'] = predicted_test\n",
    "df_nc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nc_test.predicted.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>American researcher Charles Lieber was arrest...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Did Trump Blame Obama for ‘Bad’ COVID-19 Tests...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hundreds of sampoerna cigarette factory worke...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dr. Megha Vyas from Pune died of COVID-19.</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jacob Rothschild owns a patent to coronavirus.</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2087</th>\n",
       "      <td>President Emmanuel Macron was cheered by a cr...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088</th>\n",
       "      <td>Five helicopters are going to spray the air w...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2089</th>\n",
       "      <td>Coronavirus was created in a lab in order to ...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2090</th>\n",
       "      <td>Video of a man dressed in white who recommend...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2091</th>\n",
       "      <td>President Donald Trump implemented “a travel ...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2092 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text predicted\n",
       "0      American researcher Charles Lieber was arrest...     false\n",
       "1     Did Trump Blame Obama for ‘Bad’ COVID-19 Tests...      true\n",
       "2      Hundreds of sampoerna cigarette factory worke...     false\n",
       "3            Dr. Megha Vyas from Pune died of COVID-19.     false\n",
       "4        Jacob Rothschild owns a patent to coronavirus.     false\n",
       "...                                                 ...       ...\n",
       "2087   President Emmanuel Macron was cheered by a cr...     false\n",
       "2088   Five helicopters are going to spray the air w...     false\n",
       "2089   Coronavirus was created in a lab in order to ...     false\n",
       "2090   Video of a man dressed in white who recommend...     false\n",
       "2091   President Donald Trump implemented “a travel ...     false\n",
       "\n",
       "[2092 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nc_test['predicted']=df_nc_test['predicted'].replace({0:'false',1:'true'})\n",
    "df_nc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission file\n",
    "def create_submit(df):\n",
    "    df_out=pd.DataFrame()\n",
    "    df_out['class'] = df_nc_test['predicted']\n",
    "    return df_out\n",
    "submission = create_submit(df_nc_test)\n",
    "submission.to_csv(\"submission2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export \n",
    "#su.to_csv('./data/submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to counts the number of occurrences of different tokens in a list and whole corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def build_vocabulary():\n",
    "    vocabulary = Counter()\n",
    "\n",
    "    for doc in docs:\n",
    "        words = doc.split()\n",
    "        vocabulary.update(words)\n",
    "    \n",
    "    return OrderedDict(vocabulary.most_common())\n",
    "# turn into a list of tuples and get the first 20 items\n",
    "list(build_vocabulary().items())[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Extraction from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are disabling the synctatic parser from pipeline to improve speed.\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser'])\n",
    "en_stopwords = nlp.Defaults.stop_words\n",
    "###################################################Clean data with the functions not classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verifiedby</th>\n",
       "      <th>country</th>\n",
       "      <th>text</th>\n",
       "      <th>published_date</th>\n",
       "      <th>country1</th>\n",
       "      <th>country2</th>\n",
       "      <th>country3</th>\n",
       "      <th>country4</th>\n",
       "      <th>article_source</th>\n",
       "      <th>ref_source</th>\n",
       "      <th>source_title</th>\n",
       "      <th>content_text</th>\n",
       "      <th>category</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agência Lupa</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>American researcher Charles Lieber was arrest...</td>\n",
       "      <td>2020/04/06</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://piaui.folha.uol.com.br/lupa/2020/04/06...</td>\n",
       "      <td>poynter</td>\n",
       "      <td>#Verificamos: É falso que pesquisador norte-am...</td>\n",
       "      <td>, “HOMEM QUE VENDEU O CORONA VIRUS NA CHINA, P...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>snopes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Did Trump Blame Obama for ‘Bad’ COVID-19 Tests...</td>\n",
       "      <td>2020-05-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.snopes.com/fact-check/trump-blame-...</td>\n",
       "      <td>snopes</td>\n",
       "      <td>Did Trump Blame Obama for 'Bad' COVID-19 Tests?</td>\n",
       "      <td>['U.S. President Donald Trump implied the Obam...</td>\n",
       "      <td>Trump and the Pandemic II</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEMPO</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>Hundreds of sampoerna cigarette factory worke...</td>\n",
       "      <td>2020/05/05</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://cekfakta.tempo.co/fakta/765/fakta-atau...</td>\n",
       "      <td>poynter</td>\n",
       "      <td>Tempo.co</td>\n",
       "      <td>Narasi bahwa ratusan pekerja pabrik rokok Samp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FactCrescendo</td>\n",
       "      <td>India</td>\n",
       "      <td>Dr. Megha Vyas from Pune died of COVID-19.</td>\n",
       "      <td>2020/04/25</td>\n",
       "      <td>India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.malayalam.factcrescendo.com/image-...</td>\n",
       "      <td>poynter</td>\n",
       "      <td>FACT CHECK: ചിത്രത്തില്‍ കാണുന്ന സ്ത്രി പൂനയില...</td>\n",
       "      <td>The fact behind every news!, കോവിഡ്\\u200c-19 ര...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Full Fact</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Jacob Rothschild owns a patent to coronavirus.</td>\n",
       "      <td>2020/01/30</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fullfact.org/online/Rothschild-coronavirus/</td>\n",
       "      <td>poynter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       verifiedby          country  \\\n",
       "0    Agência Lupa           Brazil   \n",
       "1          snopes              NaN   \n",
       "2           TEMPO        Indonesia   \n",
       "3   FactCrescendo            India   \n",
       "4       Full Fact   United Kingdom   \n",
       "\n",
       "                                                text       published_date  \\\n",
       "0   American researcher Charles Lieber was arrest...          2020/04/06    \n",
       "1  Did Trump Blame Obama for ‘Bad’ COVID-19 Tests...  2020-05-01 00:00:00   \n",
       "2   Hundreds of sampoerna cigarette factory worke...          2020/05/05    \n",
       "3         Dr. Megha Vyas from Pune died of COVID-19.          2020/04/25    \n",
       "4     Jacob Rothschild owns a patent to coronavirus.          2020/01/30    \n",
       "\n",
       "          country1 country2 country3 country4  \\\n",
       "0           Brazil      NaN      NaN      NaN   \n",
       "1              NaN      NaN      NaN      NaN   \n",
       "2        Indonesia      NaN      NaN      NaN   \n",
       "3            India      NaN      NaN      NaN   \n",
       "4   United Kingdom      NaN      NaN      NaN   \n",
       "\n",
       "                                      article_source ref_source  \\\n",
       "0  https://piaui.folha.uol.com.br/lupa/2020/04/06...    poynter   \n",
       "1  https://www.snopes.com/fact-check/trump-blame-...     snopes   \n",
       "2  https://cekfakta.tempo.co/fakta/765/fakta-atau...    poynter   \n",
       "3  https://www.malayalam.factcrescendo.com/image-...    poynter   \n",
       "4        fullfact.org/online/Rothschild-coronavirus/    poynter   \n",
       "\n",
       "                                        source_title  \\\n",
       "0  #Verificamos: É falso que pesquisador norte-am...   \n",
       "1    Did Trump Blame Obama for 'Bad' COVID-19 Tests?   \n",
       "2                                           Tempo.co   \n",
       "3  FACT CHECK: ചിത്രത്തില്‍ കാണുന്ന സ്ത്രി പൂനയില...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                        content_text  \\\n",
       "0  , “HOMEM QUE VENDEU O CORONA VIRUS NA CHINA, P...   \n",
       "1  ['U.S. President Donald Trump implied the Obam...   \n",
       "2  Narasi bahwa ratusan pekerja pabrik rokok Samp...   \n",
       "3  The fact behind every news!, കോവിഡ്\\u200c-19 ര...   \n",
       "4                                                NaN   \n",
       "\n",
       "                    category lang  \n",
       "0                        NaN   pt  \n",
       "1  Trump and the Pandemic II   en  \n",
       "2                        NaN   id  \n",
       "3                        NaN   ml  \n",
       "4                        NaN   tl  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_nb = pd.read_csv('./data/covid19_data.csv')#, encoding='latin1')\n",
    "#df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1,inplace=True)\n",
    "###df_train_nb = df_train_nb[['class','title']]\n",
    "df_train_nb.rename(columns={\"class\":\"target\", \"title\":\"text\"},inplace=True)\n",
    "df_test= pd.read_csv('data/covid19_unlabelled_test.csv')\n",
    "\n",
    "#df_test = df_test[['title']]\n",
    "df_test.rename(columns={\"title\":\"text\"},inplace=True)\n",
    "df_test.head()\n",
    "#NOTE: If the data set is so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_nc_train = df_train_clean.copy()\n",
    "#df_nc_test = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verifiedby</th>\n",
       "      <th>country</th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>published_date</th>\n",
       "      <th>country1</th>\n",
       "      <th>country2</th>\n",
       "      <th>country3</th>\n",
       "      <th>country4</th>\n",
       "      <th>article_source</th>\n",
       "      <th>...</th>\n",
       "      <th>length</th>\n",
       "      <th>words</th>\n",
       "      <th>hasTitle</th>\n",
       "      <th>hasContent</th>\n",
       "      <th>hasLang</th>\n",
       "      <th>hasCountry1</th>\n",
       "      <th>hasCountry2</th>\n",
       "      <th>hasCountry3</th>\n",
       "      <th>hasCountry4</th>\n",
       "      <th>num_countries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Delfi Melo Detektorius (Lie Detector)</td>\n",
       "      <td>Lithuania</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>Claims that coronavirus is fake and Belarus i...</td>\n",
       "      <td>2020/05/11</td>\n",
       "      <td>Lithuania</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.delfi.lt/news/melo-detektorius/mel...</td>\n",
       "      <td>...</td>\n",
       "      <td>98</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AfricaCheck</td>\n",
       "      <td>United States, South Africa</td>\n",
       "      <td>False</td>\n",
       "      <td>Muammar Gaddafi predicted the current coronav...</td>\n",
       "      <td>2020/03/21</td>\n",
       "      <td>United States</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://africacheck.org/fbcheck/no-gaddafi-did...</td>\n",
       "      <td>...</td>\n",
       "      <td>148</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFP</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>False</td>\n",
       "      <td>Video shows quarantined people in a building ...</td>\n",
       "      <td>2020/02/17</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://u.afp.com/QuarantineChina</td>\n",
       "      <td>...</td>\n",
       "      <td>58</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FactCrescendo</td>\n",
       "      <td>India</td>\n",
       "      <td>False</td>\n",
       "      <td>A video shows a massive explosion in Wuhan, C...</td>\n",
       "      <td>2020/02/17</td>\n",
       "      <td>India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://english.factcrescendo.com/2020/02/17/t...</td>\n",
       "      <td>...</td>\n",
       "      <td>89</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newtral.es</td>\n",
       "      <td>Spain, Peru, United States, Mexico</td>\n",
       "      <td>False</td>\n",
       "      <td>Chlorine dioxide cures the coronavirus.</td>\n",
       "      <td>2020/03/25</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Peru</td>\n",
       "      <td>United States</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>https://www.newtral.es/bulo-dioxido-de-cloro-c...</td>\n",
       "      <td>...</td>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5526</th>\n",
       "      <td>AFP</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>MISLEADING</td>\n",
       "      <td>A video shows the Italian air force air show ...</td>\n",
       "      <td>2020/03/19</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://factual.afp.com/el-video-del-espectacu...</td>\n",
       "      <td>...</td>\n",
       "      <td>95</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5527</th>\n",
       "      <td>India Today</td>\n",
       "      <td>India</td>\n",
       "      <td>Mostly false</td>\n",
       "      <td>While coronavirus tests in other countries ar...</td>\n",
       "      <td>2020/03/25</td>\n",
       "      <td>India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.indiatoday.in/fact-check/story/fac...</td>\n",
       "      <td>...</td>\n",
       "      <td>117</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5528</th>\n",
       "      <td>TEMPO</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>Misleading</td>\n",
       "      <td>Vladimir Putin released 800 tigers andlions t...</td>\n",
       "      <td>2020/03/23</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://cekfakta.tempo.co/fakta/694/fakta-atau...</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5529</th>\n",
       "      <td>Factcheck.Vlaanderen</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>misleading</td>\n",
       "      <td>The cure for coronavirus can come from Antwer...</td>\n",
       "      <td>2020/05/05</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://factcheck.vlaanderen/factcheck/komt-de...</td>\n",
       "      <td>...</td>\n",
       "      <td>65</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5530</th>\n",
       "      <td>Verificado</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Misleading</td>\n",
       "      <td>Legislator from Nuevo León, Mexico says there...</td>\n",
       "      <td>2020/06/19</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://verificado.com.mx/que-tan-certero-pued...</td>\n",
       "      <td>...</td>\n",
       "      <td>85</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5531 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  verifiedby  \\\n",
       "0      Delfi Melo Detektorius (Lie Detector)   \n",
       "1                                AfricaCheck   \n",
       "2                                        AFP   \n",
       "3                              FactCrescendo   \n",
       "4                                 Newtral.es   \n",
       "...                                      ...   \n",
       "5526                                     AFP   \n",
       "5527                             India Today   \n",
       "5528                                   TEMPO   \n",
       "5529                    Factcheck.Vlaanderen   \n",
       "5530                              Verificado   \n",
       "\n",
       "                                  country        target  \\\n",
       "0                               Lithuania         FALSE   \n",
       "1             United States, South Africa         False   \n",
       "2                               Hong Kong         False   \n",
       "3                                   India         False   \n",
       "4      Spain, Peru, United States, Mexico         False   \n",
       "...                                   ...           ...   \n",
       "5526                            Argentina    MISLEADING   \n",
       "5527                                India  Mostly false   \n",
       "5528                            Indonesia    Misleading   \n",
       "5529                              Belgium    misleading   \n",
       "5530                               Mexico    Misleading   \n",
       "\n",
       "                                                   text published_date  \\\n",
       "0      Claims that coronavirus is fake and Belarus i...    2020/05/11    \n",
       "1      Muammar Gaddafi predicted the current coronav...    2020/03/21    \n",
       "2      Video shows quarantined people in a building ...    2020/02/17    \n",
       "3      A video shows a massive explosion in Wuhan, C...    2020/02/17    \n",
       "4               Chlorine dioxide cures the coronavirus.    2020/03/25    \n",
       "...                                                 ...            ...   \n",
       "5526   A video shows the Italian air force air show ...    2020/03/19    \n",
       "5527   While coronavirus tests in other countries ar...    2020/03/25    \n",
       "5528   Vladimir Putin released 800 tigers andlions t...    2020/03/23    \n",
       "5529   The cure for coronavirus can come from Antwer...    2020/05/05    \n",
       "5530   Legislator from Nuevo León, Mexico says there...    2020/06/19    \n",
       "\n",
       "            country1       country2        country3 country4  \\\n",
       "0          Lithuania            NaN             NaN      NaN   \n",
       "1      United States   South Africa             NaN      NaN   \n",
       "2          Hong Kong            NaN             NaN      NaN   \n",
       "3              India            NaN             NaN      NaN   \n",
       "4              Spain           Peru   United States   Mexico   \n",
       "...              ...            ...             ...      ...   \n",
       "5526       Argentina            NaN             NaN      NaN   \n",
       "5527           India            NaN             NaN      NaN   \n",
       "5528       Indonesia            NaN             NaN      NaN   \n",
       "5529         Belgium            NaN             NaN      NaN   \n",
       "5530          Mexico            NaN             NaN      NaN   \n",
       "\n",
       "                                         article_source  ... length words  \\\n",
       "0     https://www.delfi.lt/news/melo-detektorius/mel...  ...     98    16   \n",
       "1     https://africacheck.org/fbcheck/no-gaddafi-did...  ...    148    23   \n",
       "2                      http://u.afp.com/QuarantineChina  ...     58     9   \n",
       "3     https://english.factcrescendo.com/2020/02/17/t...  ...     89    15   \n",
       "4     https://www.newtral.es/bulo-dioxido-de-cloro-c...  ...     40     5   \n",
       "...                                                 ...  ...    ...   ...   \n",
       "5526  https://factual.afp.com/el-video-del-espectacu...  ...     95    17   \n",
       "5527  https://www.indiatoday.in/fact-check/story/fac...  ...    117    20   \n",
       "5528  https://cekfakta.tempo.co/fakta/694/fakta-atau...  ...     80    13   \n",
       "5529  https://factcheck.vlaanderen/factcheck/komt-de...  ...     65    10   \n",
       "5530  https://verificado.com.mx/que-tan-certero-pued...  ...     85    12   \n",
       "\n",
       "     hasTitle hasContent hasLang  hasCountry1  hasCountry2  hasCountry3  \\\n",
       "0           0          0       0            1            1            1   \n",
       "1           0          0       0            1            1            1   \n",
       "2           0          0       0            1            1            1   \n",
       "3           0          0       0            1            1            1   \n",
       "4           0          0       0            1            1            1   \n",
       "...       ...        ...     ...          ...          ...          ...   \n",
       "5526        0          0       0            1            1            1   \n",
       "5527        0          0       0            1            1            1   \n",
       "5528        0          0       0            1            1            1   \n",
       "5529        0          0       0            1            1            1   \n",
       "5530        0          0       0            1            1            1   \n",
       "\n",
       "      hasCountry4  num_countries  \n",
       "0               1              4  \n",
       "1               1              4  \n",
       "2               1              4  \n",
       "3               1              4  \n",
       "4               1              4  \n",
       "...           ...            ...  \n",
       "5526            1              4  \n",
       "5527            1              4  \n",
       "5528            1              4  \n",
       "5529            1              4  \n",
       "5530            1              4  \n",
       "\n",
       "[5531 rows x 25 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ad features before cleaning the data\n",
    "def Add_feature(df):\n",
    "    \"\"\"\n",
    "    Implement the three above functions in the respective order to remove html tags, punctuations and stopwords\n",
    "    Hint: Use the apply function.\n",
    "    \"\"\"\n",
    "    df_ = df.copy()\n",
    "\n",
    "    df_['length'] = df_['text'].map(len)# we test if this feature is ok or not by plotting in the next cell we can do it for all the new features if it is skew to right and left it is important otherwise not inmportant\n",
    "    df_['words'] = df_['text'].str.split().map(len)\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "#df_clean['words_not_stopword'] = df_clean['text'].apply(lambda x: len([t for t in x.split() if t not in stop_words]))\n",
    "#df_clean['commas'] = df_clean['text'].str.count(',')\n",
    "#df_clean['upper'] = df_clean['text'].map(lambda x: map(str.isupper, x)).map(sum)\n",
    "#df_clean['capitalized'] = df_clean['text'].map(lambda x: map(str.istitle, x)).map(sum)\n",
    "#get the average word length\n",
    "#df_clean['avg_word_length'] = df_clean['text'].apply(lambda x: np.mean([len(t) for t in x.split() if t not in stop_words]) if len([len(t) for t in x.split(' ') if t not in stop_words]) > 0 else 0)\n",
    "#number of adjective\n",
    "#df_clean['n_adjs'] = n_adjs\n",
    "#df_clean['positive_emojis_count'] = count_emoji_matches(pos_patterns)\n",
    "#df_clean['negative_emojis_count'] = count_emoji_matches(neg_patterns)\n",
    "    df_['hasTitle'] = 0\n",
    "    df_.loc[df_['source_title'].isna(),'hasTitle'] = 1\n",
    "    df_['hasContent'] = 0\n",
    "    df_.loc[df_['content_text'].isna(),'hasContent'] = 1\n",
    "    df_['hasLang'] = 0\n",
    "    df_.loc[df.lang.isna(),'hasLang'] = 1\n",
    "    df_['hasCountry1'] = 0\n",
    "    df_.loc[~df.lang.isna(),'hasCountry1'] = 1\n",
    "    df_['hasCountry2'] = 0\n",
    "    df_.loc[~df.lang.isna(),'hasCountry2'] = 1\n",
    "    df_['hasCountry3'] = 0\n",
    "    df_.loc[~df.lang.isna(),'hasCountry3'] = 1\n",
    "    df_['hasCountry4'] = 0\n",
    "    df_.loc[~df.lang.isna(),'hasCountry4'] = 1\n",
    "    df_['num_countries'] = df_['hasCountry1']+df_['hasCountry2']+df_['hasCountry3']+df_['hasCountry4']\n",
    "    return df_\n",
    "\n",
    "df_nc_train = Add_feature(df_train_nb)\n",
    "df_nc_test= Add_feature(df_test)\n",
    "\n",
    "df_nc_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = preprocessing_(df_nc_train)\n",
    "df_clean_test=preprocessing_(df_nc_test)\n",
    "\n",
    "# define docs as a list of sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['verifiedby', 'country', 'target', 'text', 'published_date', 'country1',\n",
       "       'country2', 'country3', 'country4', 'article_source', 'ref_source',\n",
       "       'source_title', 'content_text', 'category', 'lang', 'length', 'words',\n",
       "       'hasTitle', 'hasContent', 'hasLang', 'hasCountry1', 'hasCountry2',\n",
       "       'hasCountry3', 'hasCountry4', 'num_countries'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_prep(fd_):    \n",
    "    df_.drop(['verifiedby', 'country', 'published_date', 'country1',\n",
    "       'country2', 'country3', 'country4', 'article_source', 'ref_source',\n",
    "       'source_title', 'content_text', 'category', 'lang'], axis=1,inplace=True)\n",
    "    return df_\n",
    "df_clean = func_prep(df_clean)\n",
    "df_clean_test = func_prep(df_clean_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(df_clean.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are going to use the function pipe to process all documents.\n",
    "# One of the strenghts for SpaCy is the parallel processing using all your computer cores.\n",
    "# In this step, SpaCy performs the NLP pipeline for all the docs, so it may take a while\n",
    "\n",
    "docs = list(nlp.pipe(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Matcher ans pass vocabulary to it\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab) # Pass the vocabulary object to Matcher.__init__()\n",
    "pattern = [{'POS': 'ADV'},{'POS':'ADJ'}]\n",
    "matcher.add('word',None,pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Extracting adjectives,\n",
    "for i, doc in enumerate(docs[:400]):\n",
    "    #print(i,doc)\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]  # the matched span\n",
    "        print(i, start, end, span)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i, doc in enumerate(docs[:400]):\n",
    "    for e in doc.ents:\n",
    "        if e.label_ == 'GPE':   \n",
    "            print(i, e.text, e.start_char, e.end_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "#\n",
    "pattern = [{'ENT_TYPE':'GPE'}]\n",
    "matcher.add('LOC', None, pattern)\n",
    "#matcher.add(...)\n",
    "most_common_ents_list = []\n",
    "for doc in docs:\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]  # the matched span\n",
    "        span_text = span.text  # the span as a string\n",
    "        most_common_ents_list.append(span_text)\n",
    "most_common_ents = Counter(most_common_ents_list).most_common(10)\n",
    "most_common_ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding even more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of adjectives\n",
    "#name_of__adjs = [[token for token in doc if token.pos_ == 'ADJ'] for doc in docs]\n",
    "#n_adjs = [len(internal_list) for internal_list in name_of__adjs]\n",
    "#####df['n_adjs'] = n_adjs\n",
    "##Emoji\n",
    "\n",
    "nlp = English()  # We only want the tokenizer, so no need to load a model\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "#pos_patterns = [{'TEXT':{\"REGEX\": r\"\\:\\)\"}}] #- For Positive emoji let's use \":)\"\n",
    "#neg_patterns = [{'TEXT':{\"REGEX\": r\"\\:\\(\"}}] #- For Negative emoji let's use \":(\"\n",
    "\n",
    "# Hint - Don't forget to escape the special character \"(\" and \")\"\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "\n",
    "#def count_emoji_matches(pattern, docs = docs):\n",
    " #   matcher = Matcher(nlp.vocab)\n",
    "  #  matcher.add(\"EMOJIS\", None, pattern)\n",
    "    \n",
    "   # n_emojis = []\n",
    "    #for doc in docs:\n",
    "     #   matches = matcher(doc)\n",
    "      #  emojis_count = len(matches)\n",
    "       # for match in matches:\n",
    "        #    emojis_count += 1\n",
    "        #n_emojis.append(emojis_count)\n",
    "            \n",
    "  #  return n_emojis\n",
    "\n",
    "#positive_emojis_count = sum(count_emoji_matches(pos_patterns))\n",
    "#negative_emojis_count = sum(count_emoji_matches(neg_patterns))\n",
    "\n",
    "\n",
    "\n",
    "##df['positive_emojis_count'] = count_emoji_matches(pos_patterns)\n",
    "##df['negative_emojis_count'] = count_emoji_matches(neg_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding more extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['length'] = df_clean['text'].map(len)# we test if this feature is ok or not by plotting in the next cell we can do it for all the new features if it is skew to right and left it is important otherwise not inmportant\n",
    "df_clean['words'] = df_clean['text'].str.split().map(len)\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "#df_clean['words_not_stopword'] = df_clean['text'].apply(lambda x: len([t for t in x.split() if t not in stop_words]))\n",
    "#df_clean['commas'] = df_clean['text'].str.count(',')\n",
    "#df_clean['upper'] = df_clean['text'].map(lambda x: map(str.isupper, x)).map(sum)\n",
    "#df_clean['capitalized'] = df_clean['text'].map(lambda x: map(str.istitle, x)).map(sum)\n",
    "#get the average word length\n",
    "#df_clean['avg_word_length'] = df_clean['text'].apply(lambda x: np.mean([len(t) for t in x.split() if t not in stop_words]) if len([len(t) for t in x.split(' ') if t not in stop_words]) > 0 else 0)\n",
    "#number of adjective\n",
    "#df_clean['n_adjs'] = n_adjs\n",
    "#df_clean['positive_emojis_count'] = count_emoji_matches(pos_patterns)\n",
    "#df_clean['negative_emojis_count'] = count_emoji_matches(neg_patterns)\n",
    "df_['hasTitle'] = 0\n",
    "df_.loc[df.source_title.isna(),'hasTitle'] = 1\n",
    "df_['hasContent'] = 0\n",
    "df_.loc[df.content_text.isna(),'hasContent'] = 1\n",
    "df_['hasLang'] = 0\n",
    "df_.loc[df.lang.isna(),'hasLang'] = 1\n",
    "df_['hasCountry1'] = 0\n",
    "df_.loc[~df.lang.isna(),'hasCountry1'] = 1\n",
    "df_['hasCountry2'] = 0\n",
    "df_.loc[~df.lang.isna(),'hasCountry2'] = 1\n",
    "df_['hasCountry3'] = 0\n",
    "df_.loc[~df.lang.isna(),'hasCountry3'] = 1\n",
    "df_['hasCountry4'] = 0\n",
    "df_.loc[~df.lang.isna(),'hasCountry4'] = 1\n",
    "df_['num_countries'] = df_['hasCountry1']+df_['hasCountry2']+df_['hasCountry3']+df_['hasCountry4']\n",
    "df_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_(df):\n",
    "    \"\"\"\n",
    "    Implement the three above functions in the respective order to remove html tags, punctuations and stopwords\n",
    "    Hint: Use the apply function.\n",
    "    \"\"\"\n",
    "    df_ = df.copy()\n",
    "############################################## ADD Features ######################\n",
    "    df_['hasTitle'] = 0\n",
    "    df_.loc[df.source_title.isna(),'hasTitle'] = 1\n",
    "    df_['hasContent'] = 0\n",
    "    df_.loc[df.content_text.isna(),'hasContent'] = 1\n",
    "    df_['hasLang'] = 0\n",
    "    df_.loc[df.lang.isna(),'hasLang'] = 1\n",
    "    df_['hasCountry1'] = 0\n",
    "    df_.loc[~df.lang.isna(),'hasCountry1'] = 1\n",
    "    df_['hasCountry2'] = 0\n",
    "    df_.loc[~df.lang.isna(),'hasCountry2'] = 1\n",
    "    df_['hasCountry3'] = 0\n",
    "    df_.loc[~df.lang.isna(),'hasCountry3'] = 1\n",
    "    df_['hasCountry4'] = 0\n",
    "    df_.loc[~df.lang.isna(),'hasCountry4'] = 1\n",
    "    df_['num_countries'] = df_['hasCountry1']+df_['hasCountry2']+df_['hasCountry3']+df_['hasCountry4']\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " df_clean.isnull.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.drop(['verifiedby', 'country', 'published_date', 'country1',\n",
    "       'country2', 'country3', 'country4', 'article_source', 'ref_source',\n",
    "       'source_title', 'content_text', 'category', 'lang'], axis=1,inplace=True)\n",
    "df_clean_test = df_clean_test.drop(['verifiedby', 'country', 'target', 'text', 'published_date', 'country1',\n",
    "       'country2', 'country3', 'country4', 'article_source', 'ref_source',\n",
    "       'source_title', 'content_text', 'category', 'lang'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is this feature usefull?(by the plot the length is  useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#length column\n",
    "ax_list = df_clean.hist(column='length', by='target', bins=50,figsize=(12,4))\n",
    "ax_list[0].set_xlim((0,300))\n",
    "ax_list[1].set_xlim((0,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_list = df_clean.hist(column='words', by='target', bins=50,figsize=(12,4))\n",
    "ax_list[0].set_xlim((0,300))\n",
    "ax_list[1].set_xlim((0,300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class for choosing extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Selector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a column from the dataframe to perform additional transformations on\n",
    "    \"\"\" \n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "\n",
    "class TextSelector(Selector):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on text columns in the data\n",
    "    \"\"\"\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "    \n",
    "    \n",
    "class NumberSelector(Selector):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = Pipeline([\n",
    "                ('selector', TextSelector(\"text\")),\n",
    "                ('tfidf', TfidfVectorizer())\n",
    "            ])\n",
    "\n",
    "length =  Pipeline([\n",
    "                ('selector', NumberSelector(\"length\")),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "words =  Pipeline([\n",
    "                ('selector', NumberSelector(key='words')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "words_not_stopword =  Pipeline([\n",
    "                ('selector', NumberSelector(key='words_not_stopword')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "#avg_word_length =  Pipeline([\n",
    " #               ('selector', NumberSelector(key='avg_word_length')),\n",
    "  #              ('standard', StandardScaler())\n",
    "   #         ])\n",
    "commas =  Pipeline([\n",
    "                ('selector', NumberSelector(key='commas')),\n",
    "                ('standard', StandardScaler()),\n",
    "            ])\n",
    "upper =  Pipeline([\n",
    "                ('selector', NumberSelector(key='upper')),\n",
    "                ('standard', StandardScaler()),\n",
    "            ])\n",
    "capitalized =  Pipeline([\n",
    "                ('selector', NumberSelector(key='capitalized')),\n",
    "                ('standard', StandardScaler()),\n",
    "            ])\n",
    "\n",
    "#n_adjs = Pipeline(['selector', NumberSelector(key='n_adjs'),\n",
    "                  # ('standard', StandardScaler()),\n",
    "                 # ])\n",
    "\n",
    "positive_emojis_count  = Pipeline([\n",
    "                ('selector', NumberSelector(\"positive_emojis_count\"))\n",
    "            ])\n",
    "negative_emojis_count = Pipeline([\n",
    "               ('selector', NumberSelector(\"negative_emojis_count\"))\n",
    "            ])\n",
    "\n",
    "\n",
    "feats = FeatureUnion([('text', text), \n",
    "                      ('length', length),\n",
    "                      ('words', words),\n",
    "                      ('words_not_stopword', words_not_stopword),\n",
    "                      #('avg_word_length', avg_word_length),\n",
    "                      ('commas', commas),\n",
    "                     ('upper', upper),\n",
    "                     ('capitalized', capitalized),\n",
    "                     #('n_adjs',n_adjs),\n",
    "                      ('positive_emojis_count',positive_emojis_count),\n",
    "                      ('negative_emojis_count',negative_emojis_count),\n",
    "                     ])\n",
    "\n",
    "feature_processing = Pipeline([('feats', feats)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_clean = df_clean.reset_index(drop = True)\n",
    "#df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train and test of clean data and fitting the model and predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in train and validation\n",
    "train_cl_data, test_cl_data = train_test_split(df_clean, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cl_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features',feats),\n",
    "    ('classifier', RandomForestClassifier(random_state = 42)),\n",
    "    #('classifier', LinearRegression()),\n",
    "])\n",
    "\n",
    "pipeline.fit(train_cl_data, train_cl_data.target)\n",
    "\n",
    "preds = pipeline.predict(test_cl_data)\n",
    "np.mean(preds == test_cl_data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to reduce the features and choose the more important one\n",
    "1- Feature Selection from chi-squared\n",
    "2-Feature selection with SVD and PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # The following will plot the  most dependent features from the chi-squared values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\") \n",
    "stopword_list =list(nltk.corpus.stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess to run the following cell about chi_values\n",
    "#df is already clean data\n",
    "#Encode the lables y_train and y_dev\n",
    "vec = TfidfVectorizer(ngram_range=(1,2))\n",
    "X_train_vec = vec.fit_transform(train_cl_data.text)\n",
    "X_test_vec = vec.transform(test_cl_data.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_values, p_values = chi2(X_train_vec, train_cl_data.target.values)\n",
    "print(chi_values, p_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vec.get_feature_names()\n",
    "\n",
    "cla()   # Clear axis\n",
    "close() # Close a figure window\n",
    "\n",
    "figure(figsize=(12,10))\n",
    "zipped_chi_squared = zip(feature_names, chi_values)\n",
    "sorted_chi_values = sorted(zipped_chi_squared, key=lambda x:x[1]) \n",
    "top_chi_values = list(zip(*sorted_chi_values[-30:]))\n",
    "\n",
    "x = range(len(top_chi_values[1]))\n",
    "labels = top_chi_values[0]\n",
    "barh(x, list(top_chi_values)[1], align='center', alpha=.2, color='g')\n",
    "yticks(x, labels)\n",
    "xlabel('$\\chi^2$')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch2 choose best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch2 = SelectKBest(chi2, k=10)\n",
    "ch2.fit(X_train_vec, train_cl_data.target)\n",
    "\n",
    "most_important_features = [feature_names[i] for i in ch2.get_support(indices=True)]\n",
    "for f in most_important_features:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_chi = ch2.transform(X_train_vec)\n",
    "X_test_chi = ch2.transform(X_test_vec)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "%timeit clf.fit(X_train_chi, train_cl_data.target)\n",
    "\n",
    "y_pred = clf.predict(X_test_chi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
