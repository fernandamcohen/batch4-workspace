{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "\n",
    "from collections import Counter, OrderedDict\n",
    "from sklearn.base import TransformerMixin,BaseEstimator\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import math\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import string\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.svm import SVC\n",
    "import spacy\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from spacy.lang.en import English\n",
    "\n",
    "from pylab import barh,plot,yticks,show,grid,xlabel,figure,cla,close\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". - matches any character, except newline.\n",
    "\n",
    "\\d, \\s \\S - match digit, match whitespace, not whitespace.\n",
    "\n",
    "\\b, \\B - word, not word boundary.\n",
    "\n",
    "[xyz] - matches x, y or z.\n",
    "\n",
    "[^xyz] - matches anything that is not x, y or z.\n",
    "\n",
    "[x-z] - matches a character between x and z.\n",
    "\n",
    "^xyz$ - ^ is the start of the string, $ is the end of the string.\n",
    "\n",
    "\\. - use escaping to match special characters.\n",
    "\n",
    "\\t, \\n - matches tab and newline.\n",
    "\n",
    "x* - matches 0 or more symbols x.\n",
    "\n",
    "x+ - matches 1 or more symbols x.\n",
    "\n",
    "x? - matches 0 or 1 symbol x.\n",
    "\n",
    ".?, *?, +?, etc - represent non-greedy search.\n",
    "\n",
    "x{5} - matches exactly 5 symbols x.\n",
    "\n",
    "x{5,} - matches 5 or more symbols x.\n",
    "\n",
    "x{5, 8} - matches between 5 and 8 symbols x.\n",
    "\n",
    "xy|yz - matches xy or yz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "#stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "#stems = [list(map(stemmer.stem, words))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "##there are two columns in the data set sentiment which is positive and negative and text columns this is movie review\n",
    "#df = pd.read_csv('./datasets/twitter.csv')\n",
    "df = pd.read_csv('./data/covid19_data.csv', encoding='latin1')\n",
    "#df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1,inplace=True)\n",
    "df = df[['class','title']]\n",
    "df.rename(columns={\"class\":\"target\", \"title\":\"text\"},inplace=True)\n",
    "\n",
    "df.head()\n",
    "#NOTE: If the data set is so many choose part of that in order to accelerate the process of analysing\n",
    "    ##docs = df.text[:200]\n",
    "# Get the text\n",
    "#docs = df['text']\n",
    "\n",
    "\n",
    "\n",
    "###we can read from file and represent it as list as follows\n",
    "#def file_to_list(file_name):\n",
    "    #with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        #return [line.strip() for line in f.readlines()]\n",
    "    \n",
    "#X_train_pre = file_to_list('data/tweets_train_preprocessed.txt')\n",
    "#X_dev_pre = file_to_list('data/tweets_dev_preprocessed.txt')\n",
    "#X_test_pre = file_to_list('data/tweets_test_preprocessed.txt')\n",
    "\n",
    "##############################################################Read a JSON file################################\n",
    "\n",
    "#docs = []\n",
    "#with open('./datasets/sample_data.json') as fp:\n",
    "    #for line in fp:\n",
    "        #entry = json.loads(line)\n",
    "        #docs.append(entry['body'])\n",
    "        \n",
    "#print('I read {} documents'.format(len(docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop some columns\n",
    "dr = ['Unverified','Mixed','Collections','Fake news','MiSLEADING','IN DISPUTE', 'Misinformation / Conspiracy theory','PANTS ON FIRE'\n",
    "   ,'Suspicions','NO EVIDENCE',]\n",
    "df.drop(df['class'] is in [], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to clean target and transform it to false and true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clean_col_target(df_):\n",
    "    for targ in df_['target'].values:\n",
    "        if targ is in ['False','false', 'FALSE','partly false', 'Mostly false', 'Partly false', 'PARTLY FALSE','Mostly False', '\n",
    "       'News', 'Partly False', 'Misleading/False', 'Mixture',\n",
    "       'no evidence', 'MOSTLY FALSE', 'Misattributed', 'Unproven',\n",
    "       'Miscaptioned', 'Fake', 'Half True', 'PARTLY TRUE', 'Not true',\n",
    "       'Scam', \"(Org. doesn't apply rating)\", 'Partially false',\n",
    "       'MOSTLY TRUE', 'Partly true', 'mislEADING', 'NO EVIDENCE',\n",
    "       'half true', 'false and misleading', 'mostly false', nan,\n",
    "       'HALF TRUE', 'Two Pinocchios', 'Suspicions', 'Partly FALSE',\n",
    "       'PANTS ON FIRE'\n",
    "       'Unlikely']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding the target coulmns to 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df['target'].values)\n",
    "df['target'] = le.transform(df['target'].values)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split raw data without cleaning to train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in train and validation\n",
    "train_nc_df, validation_nc_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_nc_df.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer to implement sentence cleaning\n",
    "class TextCleanerTransformer(TransformerMixin):\n",
    "    def __init__(self, tokenizer, stemmer, regex_list,\n",
    "                 lower=True, remove_punct=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stemmer = stemmer\n",
    "        self.regex_list = regex_list\n",
    "        self.lower = lower\n",
    "        self.remove_punct = remove_punct\n",
    "        \n",
    "    def transform(self, X, *_):\n",
    "        X = list(map(self._clean_sentence, X))\n",
    "        return X\n",
    "    \n",
    "    def _clean_sentence(self, sentence):\n",
    "        \n",
    "        # Replace given regexes\n",
    "        for regex in self.regex_list:\n",
    "            sentence = re.sub(regex[0],regex[1], sentence)\n",
    "            \n",
    "        # lowercase\n",
    "        if self.lower:\n",
    "            sentence = sentence.lower()\n",
    "\n",
    "        # Split sentence into list of words\n",
    "        words = self.tokenizer.tokenize(sentence)\n",
    "            \n",
    "        # Remove punctuation\n",
    "        if self.remove_punct:\n",
    "            words = list(filter(lambda x: x not in string.punctuation, words))\n",
    "\n",
    "        # Stem words\n",
    "        if self.stemmer:\n",
    "            words = map(self.stemmer.stem, words)\n",
    "\n",
    "        # Join list elements into string\n",
    "        sentence = \" \".join(words)\n",
    "        \n",
    "        return sentence\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data by function not class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text)\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_punct(text):\n",
    "    #remove everything except words, digits and space\n",
    "    text = re.sub(r'[^\\w\\s]','',text) \n",
    "        \n",
    "    #regex often miss the underscore so let's remove that as well\n",
    "    text = re.sub(r'\\_','',text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text, stopwords):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [tok.lower() for tok in tokens]\n",
    "    if stopwords:\n",
    "        tokens = [tok for tok in tokens if tok not in stopwords]\n",
    "    tokens = [stemmer.stem(tok) for tok in tokens]\n",
    "\n",
    "    text_processed = ' '.join(tokens)\n",
    "    return text_processed\n",
    "\n",
    "\n",
    "\n",
    "def preprocessing_(df):\n",
    "    \"\"\"\n",
    "    Implement the three above functions in the respective order to remove html tags, punctuations and stopwords\n",
    "    Hint: Use the apply function.\n",
    "    \n",
    "    \"\"\"\n",
    "    df_ = df.copy()\n",
    "    \n",
    "    #df_['text'] = df_['text'].apply(...).apply(...).apply(...)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    df_['text'] = df_['text'].apply(remove_html_tags).apply(remove_punct).apply(remove_stopwords,stopwords=en_stopwords)\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a tokenizer and a stemmer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "#regex_list = [(\"<[^>]*>\", \"\")]\n",
    "regex_list = [(\"\\#\",\"\")]\n",
    "\n",
    "#cleaner = TextCleanerTransformer(tokenizer, stemmer, regex_list)\n",
    "#docs = cleaner.transform(train_df.text.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define pipline and predict MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build the pipeline\n",
    "text_clf = Pipeline([('prep', TextCleanerTransformer(tokenizer, stemmer, regex_list)),\n",
    "                   ('vect', CountVectorizer(stop_words='english', ngram_range=(1,2))),\n",
    "                   ('tfidf', TfidfTransformer()),\n",
    "                   ('clf', MultinomialNB())])\n",
    "# Train the classifier\n",
    "##text_clf.fit(map(str, train_df['text'].values), train_df['sentiment'].values)\n",
    "text_clf.fit(map(str, train_nc_df['text'].values), train_nc_df['target'].values)\n",
    "\n",
    "##predicted = text_clf.predict(map(str, validation_df['text'].values))\n",
    "##np.mean(predicted == validation_df['sentiment'])\n",
    "\n",
    "predicted = text_clf.predict(map(str, validation_nc_df['text'].values))\n",
    "np.mean(predicted == validation_nc_df['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define pipeline and predict RandomForrestcalssifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the pipeline\n",
    "text_clf = Pipeline([('prep', TextCleanerTransformer(tokenizer, stemmer, regex_list)),\n",
    "                   ('vect', CountVectorizer(stop_words='english', ngram_range=(1,2))),\n",
    "                   ('tfidf', TfidfTransformer()),\n",
    "                   ('classifier', RandomForestClassifier(random_state = 42))])\n",
    "# Train the classifier\n",
    "##text_clf.fit(map(str, train_df['text'].values), train_df['sentiment'].values)\n",
    "text_clf.fit(map(str, train_nc_df['text'].values), train_nc_df['target'].values)\n",
    "\n",
    "##predicted = text_clf.predict(map(str, validation_df['text'].values))\n",
    "##np.mean(predicted == validation_df['sentiment'])\n",
    "\n",
    "predicted = text_clf.predict(map(str, validation_nc_df['text'].values))\n",
    "np.mean(predicted == validation_nc_df['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check classification result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the results\n",
    "##print(classification_report(y_dev, y_dev_pred))\n",
    "print(classification_report(validation_nc_df['target'].values,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to counts the number of occurrences of different tokens in a list and whole corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def build_vocabulary():\n",
    "    vocabulary = Counter()\n",
    "\n",
    "    for doc in docs:\n",
    "        words = doc.split()\n",
    "        vocabulary.update(words)\n",
    "    \n",
    "    return OrderedDict(vocabulary.most_common())\n",
    "# turn into a list of tuples and get the first 20 items\n",
    "list(build_vocabulary().items())[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Extraction from text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we are disabling the synctatic parser from pipeline to improve speed.\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser'])\n",
    "en_stopwords = nlp.Defaults.stop_words\n",
    "###################################################Clean data with the functions not classes\n",
    "df_clean = preprocessing_(df)\n",
    "df_clean\n",
    "\n",
    "# define docs as a list of sentences\n",
    "\n",
    "docs = list(df_clean.text.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#We are going to use the function pipe to process all documents.\n",
    "# One of the strenghts for SpaCy is the parallel processing using all your computer cores.\n",
    "# In this step, SpaCy performs the NLP pipeline for all the docs, so it may take a while\n",
    "\n",
    "#docs = list(nlp.pipe(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs[0].ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Matcher ans pass vocabulary to it\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab) # Pass the vocabulary object to Matcher.__init__()\n",
    "pattern = [{'POS': 'ADV'},{'POS':'ADJ'}]\n",
    "matcher.add('word',None,pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Extracting adjectives,\n",
    "for i, doc in enumerate(docs[:400]):\n",
    "    #print(i,doc)\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]  # the matched span\n",
    "        print(i, start, end, span)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i, doc in enumerate(docs[:400]):\n",
    "    for e in doc.ents:\n",
    "        if e.label_ == 'GPE':   \n",
    "            print(i, e.text, e.start_char, e.end_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "#\n",
    "pattern = [{'ENT_TYPE':'GPE'}]\n",
    "matcher.add('LOC', None, pattern)\n",
    "#matcher.add(...)\n",
    "most_common_ents_list = []\n",
    "for doc in docs:\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]  # the matched span\n",
    "        span_text = span.text  # the span as a string\n",
    "        most_common_ents_list.append(span_text)\n",
    "most_common_ents = Counter(most_common_ents_list).most_common(10)\n",
    "most_common_ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding even more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of adjectives\n",
    "name_of__adjs = [[token for token in doc if token.pos_ == 'ADJ'] for doc in docs]\n",
    "n_adjs = [len(internal_list) for internal_list in name_of__adjs]\n",
    "#####df['n_adjs'] = n_adjs\n",
    "##Emoji\n",
    "\n",
    "nlp = English()  # We only want the tokenizer, so no need to load a model\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pos_patterns = [{'TEXT':{\"REGEX\": r\"\\:\\)\"}}] #- For Positive emoji let's use \":)\"\n",
    "neg_patterns = [{'TEXT':{\"REGEX\": r\"\\:\\(\"}}] #- For Negative emoji let's use \":(\"\n",
    "\n",
    "# Hint - Don't forget to escape the special character \"(\" and \")\"\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "\n",
    "def count_emoji_matches(pattern, docs = docs):\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"EMOJIS\", None, pattern)\n",
    "    \n",
    "    n_emojis = []\n",
    "    for doc in docs:\n",
    "        matches = matcher(doc)\n",
    "        emojis_count = len(matches)\n",
    "        for match in matches:\n",
    "            emojis_count += 1\n",
    "        n_emojis.append(emojis_count)\n",
    "            \n",
    "    return n_emojis\n",
    "\n",
    "positive_emojis_count = sum(count_emoji_matches(pos_patterns))\n",
    "negative_emojis_count = sum(count_emoji_matches(neg_patterns))\n",
    "\n",
    "\n",
    "\n",
    "##df['positive_emojis_count'] = count_emoji_matches(pos_patterns)\n",
    "##df['negative_emojis_count'] = count_emoji_matches(neg_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding more extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['length'] = df_clean['text'].map(len)# we test if this feature is ok or not by plotting in the next cell we can do it for all the new features if it is skew to right and left it is important otherwise not inmportant\n",
    "df_clean['words'] = df_clean['text'].str.split().map(len)\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "df_clean['words_not_stopword'] = df_clean['text'].apply(lambda x: len([t for t in x.split() if t not in stop_words]))\n",
    "df_clean['commas'] = df_clean['text'].str.count(',')\n",
    "df_clean['upper'] = df_clean['text'].map(lambda x: map(str.isupper, x)).map(sum)\n",
    "df_clean['capitalized'] = df_clean['text'].map(lambda x: map(str.istitle, x)).map(sum)\n",
    "#get the average word length\n",
    "#df_clean['avg_word_length'] = df_clean['text'].apply(lambda x: np.mean([len(t) for t in x.split() if t not in stop_words]) if len([len(t) for t in x.split(' ') if t not in stop_words]) > 0 else 0)\n",
    "#number of adjective\n",
    "df_clean['n_adjs'] = n_adjs\n",
    "df_clean['positive_emojis_count'] = count_emoji_matches(pos_patterns)\n",
    "df_clean['negative_emojis_count'] = count_emoji_matches(neg_patterns)\n",
    "df_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is this feature usefull?(by the plot the length is  useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_list = df_clean.hist(column='length', by='target', bins=50,figsize=(12,4))\n",
    "ax_list[0].set_xlim((0,300))\n",
    "ax_list[1].set_xlim((0,300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class for choosing extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Selector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a column from the dataframe to perform additional transformations on\n",
    "    \"\"\" \n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "\n",
    "class TextSelector(Selector):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on text columns in the data\n",
    "    \"\"\"\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "    \n",
    "    \n",
    "class NumberSelector(Selector):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = Pipeline([\n",
    "                ('selector', TextSelector(\"text\")),\n",
    "                ('tfidf', TfidfVectorizer())\n",
    "            ])\n",
    "\n",
    "length =  Pipeline([\n",
    "                ('selector', NumberSelector(\"length\")),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "words =  Pipeline([\n",
    "                ('selector', NumberSelector(key='words')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "words_not_stopword =  Pipeline([\n",
    "                ('selector', NumberSelector(key='words_not_stopword')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "#avg_word_length =  Pipeline([\n",
    " #               ('selector', NumberSelector(key='avg_word_length')),\n",
    "  #              ('standard', StandardScaler())\n",
    "   #         ])\n",
    "commas =  Pipeline([\n",
    "                ('selector', NumberSelector(key='commas')),\n",
    "                ('standard', StandardScaler()),\n",
    "            ])\n",
    "upper =  Pipeline([\n",
    "                ('selector', NumberSelector(key='upper')),\n",
    "                ('standard', StandardScaler()),\n",
    "            ])\n",
    "capitalized =  Pipeline([\n",
    "                ('selector', NumberSelector(key='capitalized')),\n",
    "                ('standard', StandardScaler()),\n",
    "            ])\n",
    "\n",
    "#n_adjs = Pipeline(['selector', NumberSelector(key='n_adjs'),\n",
    "                  # ('standard', StandardScaler()),\n",
    "                 # ])\n",
    "\n",
    "positive_emojis_count  = Pipeline([\n",
    "                ('selector', NumberSelector(\"positive_emojis_count\"))\n",
    "            ])\n",
    "negative_emojis_count = Pipeline([\n",
    "               ('selector', NumberSelector(\"negative_emojis_count\"))\n",
    "            ])\n",
    "\n",
    "\n",
    "feats = FeatureUnion([('text', text), \n",
    "                      ('length', length),\n",
    "                      ('words', words),\n",
    "                      ('words_not_stopword', words_not_stopword),\n",
    "                      #('avg_word_length', avg_word_length),\n",
    "                      ('commas', commas),\n",
    "                     ('upper', upper),\n",
    "                     ('capitalized', capitalized),\n",
    "                     #('n_adjs',n_adjs),\n",
    "                      ('positive_emojis_count',positive_emojis_count),\n",
    "                      ('negative_emojis_count',negative_emojis_count),\n",
    "                     ])\n",
    "\n",
    "feature_processing = Pipeline([('feats', feats)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_clean = df_clean.reset_index(drop = True)\n",
    "#df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train and test of clean data and fitting the model and predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in train and validation\n",
    "train_cl_data, test_cl_data = train_test_split(df_clean, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cl_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features',feats),\n",
    "    ('classifier', RandomForestClassifier(random_state = 42)),\n",
    "    #('classifier', LinearRegression()),\n",
    "])\n",
    "\n",
    "pipeline.fit(train_cl_data, train_cl_data.target)\n",
    "\n",
    "preds = pipeline.predict(test_cl_data)\n",
    "np.mean(preds == test_cl_data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to reduce the features and choose the more important one\n",
    "1- Feature Selection from chi-squared\n",
    "2-Feature selection with SVD and PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # The following will plot the  most dependent features from the chi-squared values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\") \n",
    "stopword_list =list(nltk.corpus.stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess to run the following cell about chi_values\n",
    "#df is already clean data\n",
    "#Encode the lables y_train and y_dev\n",
    "vec = TfidfVectorizer(ngram_range=(1,2))\n",
    "X_train_vec = vec.fit_transform(train_cl_data.text)\n",
    "X_test_vec = vec.transform(test_cl_data.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_values, p_values = chi2(X_train_vec, train_cl_data.target.values)\n",
    "print(chi_values, p_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vec.get_feature_names()\n",
    "\n",
    "cla()   # Clear axis\n",
    "close() # Close a figure window\n",
    "\n",
    "figure(figsize=(12,10))\n",
    "zipped_chi_squared = zip(feature_names, chi_values)\n",
    "sorted_chi_values = sorted(zipped_chi_squared, key=lambda x:x[1]) \n",
    "top_chi_values = list(zip(*sorted_chi_values[-30:]))\n",
    "\n",
    "x = range(len(top_chi_values[1]))\n",
    "labels = top_chi_values[0]\n",
    "barh(x, list(top_chi_values)[1], align='center', alpha=.2, color='g')\n",
    "yticks(x, labels)\n",
    "xlabel('$\\chi^2$')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch2 choose best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch2 = SelectKBest(chi2, k=10)\n",
    "ch2.fit(X_train_vec, train_cl_data.target)\n",
    "\n",
    "most_important_features = [feature_names[i] for i in ch2.get_support(indices=True)]\n",
    "for f in most_important_features:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_chi = ch2.transform(X_train_vec)\n",
    "X_test_chi = ch2.transform(X_test_vec)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "%timeit clf.fit(X_train_chi, train_cl_data.target)\n",
    "\n",
    "y_pred = clf.predict(X_test_chi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
