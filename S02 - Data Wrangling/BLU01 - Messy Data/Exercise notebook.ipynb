{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1b6b75485105e36c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# BLU01 - Exercises Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note**\n",
    "\n",
    "Attention Windows users: The grader will run on Linux, and using power shell statements will not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0240afddd4fae69d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import chardet\n",
    "import hashlib # for grading purposes\n",
    "import math\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-93e0b4e1a40ebaaa",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Q1: Use a shell command to count lines\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-05a3d2245d57305d",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Use the file data/exercises/surfing.txt\n",
    "# Start by counting the total lines and add the total to the variable count_total.  \n",
    "# count_total = ...\n",
    "\n",
    "\n",
    "auxiliar = ! wc -l < data/exercises/surfing.txt\n",
    "count_total = int(auxiliar[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2e69c6137e73d90c",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "expected_hash = 'c6f3ac57944a531490cd39902d0f777715fd005efac9a30622d5f5205e7f6894'\n",
    "assert hashlib.sha256(str(count_total).encode()).hexdigest() == expected_hash, \"count_total is wrong\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now store in 2 variables the first third and the last third of the lines existent on that file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-14cf2619322a8763",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# NOT GRADED optional exercise!!\n",
    "# Now add the first third and the last third of the lines to the variables first_third and last_third. \n",
    "# Make it work to any files using count_total/3 for instance in the bash commands\n",
    "# first_third = ...\n",
    "\n",
    "first_third = ! head -3 data/exercises/surfing.txt\n",
    "\n",
    "# last_third = ...\n",
    "last_third = ! tail -3 data/exercises/surfing.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a6556340d1db98d7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Test the `first_third` and `last_third` exercise by running the following (ungraded) asserts.\n",
    "\n",
    "```\n",
    "assert first_third[-1][0] == 's'\n",
    "assert last_third[0][0] == 'l'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e27cb8588bb5fd40",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Q2: Read a file with specific delimiter\n",
    "\n",
    "Read file **data/exercises/surfing_sessions.csv** into a pandas DataFrame.\n",
    "\n",
    "First, you should preview the file using a shell command in order to find out the used delimiter, and other properties of this file.\n",
    "\n",
    "Then, you should use function read_csv to read the data into a DataFrame. The resulting DataFrame should have the last column as index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-942e0590467e20d9",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surfer;wave_power;location;visit_id\r\n",
      "3;37,6;Rocky Point;146097\r\n",
      "3;5,7;Chuns;146087\r\n",
      "3;5;Noronha(Cacimba);130395\r\n",
      "3;5;Rocky Point;130363\r\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surfer</th>\n",
       "      <th>wave_power</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visit_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>146097</th>\n",
       "      <td>3</td>\n",
       "      <td>37,6</td>\n",
       "      <td>Rocky Point</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146087</th>\n",
       "      <td>3</td>\n",
       "      <td>5,7</td>\n",
       "      <td>Chuns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130395</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Noronha(Cacimba)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130363</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Rocky Point</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130362</th>\n",
       "      <td>3</td>\n",
       "      <td>11,3</td>\n",
       "      <td>Noronha(Conceicao)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          surfer wave_power            location\n",
       "visit_id                                       \n",
       "146097         3       37,6         Rocky Point\n",
       "146087         3        5,7               Chuns\n",
       "130395         3          5    Noronha(Cacimba)\n",
       "130363         3          5         Rocky Point\n",
       "130362         3       11,3  Noronha(Conceicao)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a shell command to preview the data\n",
    "# ! ...\n",
    "# YOUR CODE HERE\n",
    "! head -5 data/exercises/surfing_sessions.csv\n",
    "#! cat data/exercises/surfing_sessions.csv\n",
    "\n",
    "# Use function read_csv to read the data into a DataFrame\n",
    "# df2 = pd.read_csv(...)\n",
    "# YOUR CODE HERE\n",
    "df2 = pd.read_csv('data/exercises/surfing_sessions.csv', sep=';', index_col = 'visit_id')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0c275c9acaa9c74d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert df2.loc[126903, 'location'] == 'Pupukea', \"df3 data is wrong\"\n",
    "assert set(df2.columns) == {'location', 'surfer', 'wave_power'}, \"df3 data is wrong\"\n",
    "assert len(df2) == 108, \"df3 data is wrong\"\n",
    "assert df2.index[0] == 146097, \"df3 data is wrong\"\n",
    "\n",
    "expected_hash = 'a27dd8b953f484c0d74059da102e4b8e513630292de5d17ff5585fc6743d62f0'\n",
    "assert hashlib.sha256(df2.index.name.encode()).hexdigest() == expected_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f6f9efef818e4a83",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Q3: Read a csv file with problems\n",
    "\n",
    "Read file **data/exercises/surfing_sessions_w_problems.csv** using function `read_csv`. Pay attention to the following:\n",
    "* you might find some trouble when reading the file, at first, then just ignore the problems ;)\n",
    "* use the first column as index\n",
    "* there are some inputs in the file that should be interpreted as NaN, make sure you select the right one when reading the file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2771b707556c08d2",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visit_id,surfer,wave_power,location\r\n",
      "146097,3,37.6,Rocky Point\r\n",
      "146087,3,5.7,Chuns\r\n",
      "130395,3,999999,Noronha(Cacimba)\r\n",
      "130363,3,5,Rocky Point\r\n",
      "130362,3,11.3,Noronha(Conceicao)\r\n",
      "130360,3,8.9,Noronha(Boldo)\r\n",
      "126903,3,32.7,Pupuke\r\n",
      "126902,3,no-data,Pupukea,,\r\n",
      "126591,3,9.7,Ehukai\r\n",
      "126590,3,10.4,Changes\r\n",
      "126341,3,8.2,Changes\r\n",
      "126340,3,38.6,Laniakea,\r\n",
      "125540,3,34.1,Laniakea\r\n",
      "122199,3,14.4,Glass Doors\r\n",
      "121170,3,34.9,Walter’s West\r\n",
      "120454,3,999999,Gas Chambers\r\n",
      "119736,3,21.7,Rocky Point,\r\n",
      "119735,3,21.2,Rocky Point,,\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 9: expected 4 fields, saw 6\\nSkipping line 13: expected 4 fields, saw 5\\nSkipping line 18: expected 4 fields, saw 5\\nSkipping line 19: expected 4 fields, saw 6\\n'\n"
     ]
    }
   ],
   "source": [
    "# Read file data/exercises/surfing_sessions_w_problems.csv with read_csv\n",
    "# df3 = pd.read_csv(...)\n",
    "# YOUR CODE HERE\n",
    "\n",
    "! head -20 data/exercises/surfing_sessions_w_problems.csv\n",
    "\n",
    "#\n",
    "\n",
    "df3 =  pd.read_csv('data/exercises/surfing_sessions_w_problems.csv', \n",
    "            index_col = 'visit_id',\n",
    "            na_values={'wave_power': 999999},\n",
    "            error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-601b354802964776",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.isnan(df3.loc[130395, 'wave_power']), \"df3 data is wrong\"\n",
    "assert df3.loc[126903, 'wave_power'] == 32.7, \"df3  data is wrong\"\n",
    "\n",
    "mean_selective_waste = df3['wave_power'].mean()\n",
    "assert math.isclose(17.741, mean_selective_waste, rel_tol=1e-3), \"df3 data is wrong\"\n",
    "\n",
    "expected_hash = 'a27dd8b953f484c0d74059da102e4b8e513630292de5d17ff5585fc6743d62f0'\n",
    "assert hashlib.sha256(df3.index.name.encode()).hexdigest() == expected_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5cfd5a97b1c49a0e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Q4: Repair a csv file after importing\n",
    "Read the same file **data/exercises/surfing_sessions_w_problems.csv** using function `read_csv`. But now, be sure you don't miss any lines with relevant information! \n",
    "\n",
    "* use csv module to import everything to a list of lists\n",
    "* create a df with only 4 meaningful columns and then convert the column `visit_id` to index\n",
    "* replace garbage values with NaN's \n",
    "* format the columns with the right type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-119041388e17fb72",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 18 entries, 146097 to 119735\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   surfer      18 non-null     int64  \n",
      " 1   wave_power  15 non-null     float64\n",
      " 2   location    18 non-null     object \n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 576.0+ bytes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surfer</th>\n",
       "      <th>wave_power</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visit_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>146097</th>\n",
       "      <td>3</td>\n",
       "      <td>37.6</td>\n",
       "      <td>Rocky Point</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146087</th>\n",
       "      <td>3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>Chuns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130395</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Noronha(Cacimba)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130363</th>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Rocky Point</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130362</th>\n",
       "      <td>3</td>\n",
       "      <td>11.3</td>\n",
       "      <td>Noronha(Conceicao)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          surfer  wave_power            location\n",
       "visit_id                                        \n",
       "146097         3        37.6         Rocky Point\n",
       "146087         3         5.7               Chuns\n",
       "130395         3         NaN    Noronha(Cacimba)\n",
       "130363         3         5.0         Rocky Point\n",
       "130362         3        11.3  Noronha(Conceicao)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read file data/exercises/surfing_sessions_w_problems.csv using csv.reader and add result to\n",
    "# lines = ...\n",
    "f = open('data/exercises/surfing_sessions_w_problems.csv', 'r')\n",
    "# create a list of lines\n",
    "lines = list(csv.reader(f))\n",
    "\n",
    "# create a dataframe using the line list with only 4 columns\n",
    "# df4 = pd.DataFrame(...\n",
    "# YOUR CODE HERE\n",
    "n_elements = 4\n",
    "csv_list = [i[:n_elements] for i in lines]\n",
    "f.close()\n",
    "\n",
    "df4 = pd.DataFrame(csv_list[1:], columns=csv_list[0])\n",
    "\n",
    "# replace invalid values with nan (np.nan)\n",
    "df4['wave_power'][df4['wave_power'] == '999999'] = np.nan\n",
    "df4['wave_power'][df4['wave_power'] == 'no-data'] = np.nan\n",
    "\n",
    "# set types per dataframe column (always use int64 when int's are needed)\n",
    "# YOUR CODE HERE\n",
    "\n",
    "df4[['visit_id', 'surfer']] = df4[['visit_id', 'surfer']].astype('int64') \n",
    "df4['wave_power'] = df4['wave_power'].astype(float)\n",
    "\n",
    "# set a new index to the dataframe\n",
    "df4.set_index('visit_id', inplace = True)\n",
    "\n",
    "df4.info()\n",
    "df4.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a42299f9047590d5",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.isnan(df4.loc[130395, 'wave_power']), \"df4 content is wrong\"\n",
    "assert df4.loc[126340, 'wave_power'] == 38.6, \"df4 content is wrong\"\n",
    "\n",
    "mean_wave_power = df4['wave_power'].mean()\n",
    "assert math.isclose(19.626, mean_wave_power, rel_tol=1e-3), \"df4 content is wrong\"\n",
    "\n",
    "assert df4['surfer'].dtype == np.int64, \"df4 types are wrong\"\n",
    "assert df4['location'].dtype == object, \"df4 types are wrong\"\n",
    "assert df4['wave_power'].dtype == float, \"df4 types are wrong\"\n",
    "assert df4.index.dtype == np.int64, \"df4 types are wrong\"\n",
    "\n",
    "\n",
    "expected_hash = 'a27dd8b953f484c0d74059da102e4b8e513630292de5d17ff5585fc6743d62f0'\n",
    "assert hashlib.sha256(df4.index.name.encode()).hexdigest() == expected_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-389bc42fe462c70e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Q5: Read a JSON file\n",
    "\n",
    "Read file **data/exercises/portugal_production_of_electricity_gwh.json**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5c2125479f7e50e5",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Read file data/exercises/portugal_production_of_electricity_gwh.json with read_json\n",
    "# df5 = read_json(...)\n",
    "# YOUR CODE HERE\n",
    "\n",
    "df5 = pd.read_json('data/exercises/portugal_production_of_electricity_gwh.json', orient='index')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'48beaa4bb16f0656bb4e4d3abb5da6ff6a50b52be037354f8c33455a7534102e'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashlib.sha256(str(df5.loc[:,'Hydropower > 10MW'].sum()).encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3f085fa2cdad9319",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(df5) == 23\n",
    "assert set(df5.columns) == {'Biomass', 'Geothermal power', 'Hydropower < 10MW', 'Hydropower > 10MW', \n",
    "                           'Photovoltaic', 'Total','Total renewables','Windpower','Year'}, \"df5 columns are wrong\"\n",
    "\n",
    "expected_hash = '48beaa4bb16f0656bb4e4d3abb5da6ff6a50b52be037354f8c33455a7534102e'\n",
    "assert hashlib.sha256(str(df5.loc[:,'Hydropower > 10MW'].sum()).encode()).hexdigest() == expected_hash, \"df5 content is wrong\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4d4b970f55b3e427",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Q6: Read an Excel file\n",
    "\n",
    "Read file **data/exercises/portugal_gas_emissions_per_year.xlsx** using function read_excel. Pay attention to the following:\n",
    "\n",
    "* you should grab the table \"Series\" in sheet \"Metadata\"\n",
    "* use column 'Serie' as index\n",
    "* make sure you keep only the rows and columns with data\n",
    "* set the variable distinct_scales with the number of ... distinct scales found in the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-017a7c31b0ddc38e",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Measure Unit</th>\n",
       "      <th>Value Type</th>\n",
       "      <th>Scale</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Serie</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Carbon dioxide from fossil origin</th>\n",
       "      <td>t (tonne)</td>\n",
       "      <td>Absolute Value</td>\n",
       "      <td>10^3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carbon dioxide from biomass</th>\n",
       "      <td>t (tonne)</td>\n",
       "      <td>Absolute Value</td>\n",
       "      <td>10^3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nitrous oxide</th>\n",
       "      <td>t (tonne)</td>\n",
       "      <td>Absolute Value</td>\n",
       "      <td>N.º</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Methane</th>\n",
       "      <td>t (tonne)</td>\n",
       "      <td>Absolute Value</td>\n",
       "      <td>N.º</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ammonia</th>\n",
       "      <td>t (tonne)</td>\n",
       "      <td>Absolute Value</td>\n",
       "      <td>N.º</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Measure Unit      Value Type Scale Notes\n",
       "Serie                                                                     \n",
       "Carbon dioxide from fossil origin    t (tonne)  Absolute Value  10^3   NaN\n",
       "Carbon dioxide from biomass          t (tonne)  Absolute Value  10^3   NaN\n",
       "Nitrous oxide                        t (tonne)  Absolute Value   N.º   NaN\n",
       "Methane                              t (tonne)  Absolute Value   N.º   NaN\n",
       "Ammonia                              t (tonne)  Absolute Value   N.º   NaN"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read file data/exercises/portugal_gas_emissions_per_year.xlsx with read_excel\n",
    "# df6 = read_excel(...)\n",
    "# YOUR CODE HERE\n",
    "\n",
    "df6 = pd.read_excel('data/exercises/portugal_gas_emissions_per_year.xlsx', \n",
    "              sheet_name= 'Metadata', \n",
    "              skiprows=22, \n",
    "              skipfooter=6,\n",
    "              usecols = ['Serie', 'Measure Unit','Value Type', 'Scale', 'Notes'])\n",
    "\n",
    "df6.set_index('Serie', inplace = True)\n",
    "\n",
    "distinct_scales = df6['Scale'].nunique()\n",
    "distinct_scales\n",
    "df6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-91d5f07e40585680",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert distinct_scales == 2\n",
    "assert isinstance(df6, pd.DataFrame)\n",
    "expected_hash = '60c3ad36f77e7366103fe36a3f551a0cde7d64e26d3102ddb8b953d6208a6006'\n",
    "assert hashlib.sha256(\n",
    "        df6.loc[\n",
    "            df6.index==\"Nitrogen oxides\", \n",
    "            \"Measure Unit\"][0].encode()\n",
    "    ).hexdigest() == expected_hash, \"df6 is wrong\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4a7407517d4d92c3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Q7: Find the encoding of a file\n",
    "\n",
    "Find the encoding used in file **data/exercises/cities.csv**, using the method that was shown in the Learning Units.\n",
    "\n",
    "Then, read the data into a DataFrame, using the read_csv method and find the `City` characters that has distance equal to 4765."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bd6944fd63bb38d8",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Акйс'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the encoding of file data/exercises/cities.csv\n",
    "chardet.detect(open('data/exercises/cities.csv', 'rb').read())\n",
    "encoding = 'IBM866'\n",
    "\n",
    "# Read the file into a DataFrame\n",
    "# df7 = ...\n",
    "df7 = pd.read_csv('data/exercises/cities.csv', encoding='IBM866')\n",
    "\n",
    "\n",
    "# Find the name of the city with distance = 4765\n",
    "#city_found = .... (assign a string)\n",
    "\n",
    "city_found = df7['City'][df7['distance']==4765].values[0]\n",
    "city_found\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-687fb83c97e03355",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(df7, pd.DataFrame)\n",
    "\n",
    "expected_hash_1 = '969aef39a1d4cb1c5928c774cd7a4e3ccfc064a18fbd43a70193a2631d8a122d'\n",
    "assert hashlib.sha256(encoding.encode()).hexdigest() == expected_hash_1, \"encoding is wrong\"\n",
    "\n",
    "expected_hash_2 = '8af574a768f5527ee587cae4ac76837e7ec15d9f34d9dd288c9b4d161f4ad9a2'\n",
    "assert hashlib.sha256(city_found.encode()).hexdigest() == expected_hash_2, \"city_found is wrong\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d103dfae3d5e11fe",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Q8: Import a Random Sample of a Big File\n",
    "\n",
    "Consider the file **data/exercises/world_percentage_of_literacy.tsv**. Let's imagine this file is really huge, with a lot of rows!  Read the file using a random sample of 12 rows. Count the actual lines with `wc` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fb39107093dd73fb",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Literacy rate (all)</th>\n",
       "      <th>Male literacy</th>\n",
       "      <th>Female literacy</th>\n",
       "      <th>Gender difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Albania</td>\n",
       "      <td>97.6%</td>\n",
       "      <td>98.4%</td>\n",
       "      <td>96.8%</td>\n",
       "      <td>1.6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bahamas</td>\n",
       "      <td>not reported by UNESCO 2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bahrain</td>\n",
       "      <td>95.7%</td>\n",
       "      <td>96.9%</td>\n",
       "      <td>93.5%</td>\n",
       "      <td>3.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ivory Coast</td>\n",
       "      <td>43.1%</td>\n",
       "      <td>53.1%</td>\n",
       "      <td>32.5%</td>\n",
       "      <td>20.6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Guinea</td>\n",
       "      <td>30.4%</td>\n",
       "      <td>38.1%</td>\n",
       "      <td>22.8%</td>\n",
       "      <td>15.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Iran</td>\n",
       "      <td>86.8%</td>\n",
       "      <td>91.2%</td>\n",
       "      <td>82.5%</td>\n",
       "      <td>8.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Myanmar</td>\n",
       "      <td>75.6%[note 6]</td>\n",
       "      <td>80.0%</td>\n",
       "      <td>71.9%</td>\n",
       "      <td>8.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Serbia</td>\n",
       "      <td>98.1%</td>\n",
       "      <td>99.1%</td>\n",
       "      <td>97.2%</td>\n",
       "      <td>1.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>not reported by UNESCO 2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Switzerland</td>\n",
       "      <td>not reported by UNESCO 2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Zambia</td>\n",
       "      <td>63.4%</td>\n",
       "      <td>70.9%</td>\n",
       "      <td>56.0%</td>\n",
       "      <td>14.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>86.5%</td>\n",
       "      <td>88.5%</td>\n",
       "      <td>84.6%</td>\n",
       "      <td>4.0%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Country          Literacy rate (all) Male literacy Female literacy  \\\n",
       "0       Albania                        97.6%         98.4%           96.8%   \n",
       "1       Bahamas  not reported by UNESCO 2015           NaN             NaN   \n",
       "2       Bahrain                        95.7%         96.9%           93.5%   \n",
       "3   Ivory Coast                        43.1%         53.1%           32.5%   \n",
       "4        Guinea                        30.4%         38.1%           22.8%   \n",
       "5          Iran                        86.8%         91.2%           82.5%   \n",
       "6       Myanmar                75.6%[note 6]         80.0%           71.9%   \n",
       "7        Serbia                        98.1%         99.1%           97.2%   \n",
       "8        Sweden  not reported by UNESCO 2015           NaN             NaN   \n",
       "9   Switzerland  not reported by UNESCO 2015           NaN             NaN   \n",
       "10       Zambia                        63.4%         70.9%           56.0%   \n",
       "11     Zimbabwe                        86.5%         88.5%           84.6%   \n",
       "\n",
       "   Gender difference  \n",
       "0               1.6%  \n",
       "1                NaN  \n",
       "2               3.5%  \n",
       "3              20.6%  \n",
       "4              15.3%  \n",
       "5               8.7%  \n",
       "6               8.1%  \n",
       "7               1.9%  \n",
       "8                NaN  \n",
       "9                NaN  \n",
       "10             14.9%  \n",
       "11              4.0%  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read file data/exercises/world_percentage_of_literacy.tsv with wc and save the number of lines\n",
    "# in lines_in_file (as an integer), using \n",
    "# lines_in_file = ...\n",
    "# Notice that the header is not a line..\n",
    "\n",
    "lines_in_file = ! wc -l < data/exercises/world_percentage_of_literacy.tsv\n",
    "lines_in_file = int(lines_in_file[0])-1\n",
    "lines_in_file\n",
    "\n",
    "# make parameter rows_to_skip equal to the lines you want to skip loading \n",
    "# don't forget: 12 rows should be fecthed\n",
    "# rows_to_skip = ...\n",
    "\n",
    "sample_number = 12\n",
    "n_rows_to_skip = lines_in_file - sample_number\n",
    "n_rows_to_skip\n",
    "\n",
    "# Create a df8 dataframe with the sampled values\n",
    "# don't forget: you want to keep the header plus 12 rows in the new dataframe.\n",
    "\n",
    "random.seed(42)\n",
    "rows_to_skip = random.sample(\n",
    "    range(1, lines_in_file-1), # this is a range from the first row after the header, to the last row on the file\n",
    "    n_rows_to_skip # this is the number of rows we want to random sample here, and that we will be skipped on pd.read_csv with argument skiprows\n",
    ")\n",
    "\n",
    "df8 = pd.read_csv( \n",
    "    'data/exercises/world_percentage_of_literacy.tsv',\n",
    "    sep='\\t',\n",
    "    skiprows=rows_to_skip\n",
    ")\n",
    "\n",
    "\n",
    "df8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ec15a8be42137a01",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# getting remaining list of countries, removing rows_to_skip from the file\n",
    "df_helper = pd.read_csv( \n",
    "    'data/exercises/world_percentage_of_literacy.tsv', \n",
    "    sep='\\t',\n",
    "    header=0 \n",
    ")\n",
    "\n",
    "total_indexes = list(df_helper.index)\n",
    "for s in rows_to_skip:\n",
    "    total_indexes.remove(s-1)\n",
    "\n",
    "expected_hash = '7559ca4a957c8c82ba04781cd66a68d6022229fca0e8e88d8e487c96ee4446d0'\n",
    "assert hashlib.sha256(str(lines_in_file).encode()).hexdigest() == expected_hash, \"lines_in_file are wrong\"\n",
    "assert isinstance(df8, pd.DataFrame)\n",
    "assert list(df8['Country'])==list(df_helper.iloc[total_indexes]['Country'])\n",
    "assert df8.shape[0]==12, \"df8 size is wrong\"\n",
    "\n",
    "expected_hash = '6b51d431df5d7f141cbececcf79edf3dd861c3b4069f0b11661a3eefacbba918'\n",
    "assert  hashlib.sha256(str(df8.shape[0]).encode()).hexdigest() == expected_hash\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f87e07a7b2cacd9d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Q9: Loading a Big File\n",
    "\n",
    "Read file **data/exercises/world_percentage_of_literacy.tsv** using chunks keep only the columns `Country` and `Literacy rate (all)`.\n",
    "Note that:\n",
    "* file should be read by chunks of 10 countries\n",
    "* the missing values should be removed (filtered in each chunk)\n",
    "* the `Literacy rate (all)` should be converted to type float (in each chunk)\n",
    "* the index should be incremental starting from 0 (i.e, you don't need to read any column as the index)\n",
    "\n",
    "At the end, calculate the average `Literacy rate (all)`\n",
    "\n",
    "Tip: Be sure you investigate the data you are about to load before doing any code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5106c25a705296e8",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.22333333333333\n",
      "We analyzed a total of 150 rows divided in 20  chunks with the following configuration:\n",
      " [6, 6, 10, 9, 6, 8, 7, 9, 6, 8, 8, 8, 6, 10, 6, 8, 8, 10, 7, 4]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 2 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Country              150 non-null    object \n",
      " 1   Literacy rate (all)  150 non-null    float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 2.5+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read file data/exercises/world_percentage_of_literacy.tsv\n",
    "# the chunks should be appended in a list called chunk_arr\n",
    "# YOUR CODE HERE\n",
    "#! head -10 data/exercises/world_percentage_of_literacy.tsv\n",
    "\n",
    "def adjst_literacy(data):\n",
    "    data = data.copy()\n",
    "    data = data[data['Literacy rate (all)'] != 'not reported by UNESCO 2015']\n",
    "    #data = data[data['Literacy rate (all)'] != '72.8%[note 2]']\n",
    "    #data = data[data['Literacy rate (all)'] != '97.9% (in 2012)']\n",
    "    #data = data[data['Literacy rate (all)'] != '75.6%[note 6]']\n",
    "    #data['Literacy rate (all)'][data['Literacy rate (all)'] == 'not reported by UNESCO 2015'] = np.nan\n",
    "    data['Literacy rate (all)'] = data['Literacy rate (all)'].str.split('%').str.get(0).astype(float)\n",
    "    data = data[data['Country'] != 'World']\n",
    "    return data\n",
    "    \n",
    "\n",
    "chunks_iter = pd.read_csv(\n",
    "    'data/exercises/world_percentage_of_literacy.tsv',\n",
    "    sep='\\t',\n",
    "    #na_values = {'Literacy rate (all)': 'not reported by UNESCO 2015'},\n",
    "    usecols = ['Country', 'Literacy rate (all)'],\n",
    "    chunksize=10\n",
    ")\n",
    "\n",
    "chunk_arr = []\n",
    "for data_chunk in chunks_iter:\n",
    "    data_chunk_adj = adjst_literacy(data_chunk)\n",
    "    chunk_arr.append(data_chunk_adj)\n",
    "    \n",
    "df9 = pd.concat(chunk_arr, axis=0)\n",
    "df9.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# df9 should be the final dataframe with concatenated chunks\n",
    "# Resulting average should go on lit_avg variable\n",
    "lit_avg = df9['Literacy rate (all)'].mean()\n",
    "print(lit_avg)\n",
    "\n",
    "df9.head(20)\n",
    "#df9['Country'].unique()\n",
    "\n",
    "\n",
    "print(\n",
    "    \"We analyzed a total of\",\n",
    "    sum([len(c) for c in chunk_arr]), \n",
    "    \"rows divided in\", \n",
    "    len(chunk_arr), \n",
    "    \" chunks with the following configuration:\\n\",\n",
    "    [len(c) for c in chunk_arr]\n",
    ")\n",
    "\n",
    "df9.info()\n",
    "len(chunk_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teste = pd.read_csv('data/exercises/world_percentage_of_literacy.tsv',\n",
    "            sep='\\t',\n",
    "            na_values = {'Literacy rate (all)': 'not reported by UNESCO 2015'},\n",
    "           usecols = ['Country', 'Literacy rate (all)'])\n",
    "\n",
    "teste['Literacy rate (all)'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f60a22c3465d5b83",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "error on chunk_arr sizes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-b5ff5a42dcbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mexpected_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'f26cd8ca964afd7aaaea0cacb142419676cf9928772f5b5310a036ffdae1586a'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msha256\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk_arr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhexdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mexpected_hash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"error on chunk_arr sizes\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mdf9\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf9\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Country'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'World'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Literacy rate (all)'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m86.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"df9 data is wrong\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mdf9\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Country'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"df9 structure is wrong\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: error on chunk_arr sizes"
     ]
    }
   ],
   "source": [
    "expected_hash = 'f26cd8ca964afd7aaaea0cacb142419676cf9928772f5b5310a036ffdae1586a'\n",
    "assert hashlib.sha256(str([len(c) for c in chunk_arr]).encode()).hexdigest() == expected_hash, \"error on chunk_arr sizes\"\n",
    "\n",
    "assert df9.loc[df9['Country']=='World', 'Literacy rate (all)'].values[0] == 86.3, \"df9 data is wrong\"\n",
    "assert df9.dtypes['Country'] == np.object, \"df9 structure is wrong\"\n",
    "assert df9.dtypes['Literacy rate (all)'] == np.float, \"df9 structure is wrong\" \n",
    "\n",
    "expected_hash = '94f2bba3a658b5642ebbc9b952af45c3db1a185ae0357e4fe7ced784f0c3fe29'\n",
    "assert hashlib.sha256(str(lit_avg).encode()).hexdigest() == expected_hash, \"lit_avg is wrong\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e990312f7cddd0b2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Q10: Calculate average values using chunks and avoiding a complete data frame in memory.\n",
    "\n",
    "Using chunks, read file **data/exercises/world_percentage_of_literacy.tsv**, avoid incompleted rows and calculate the average of ***Literacy rate (all)*** without loading all data simultaneously. \n",
    "\n",
    "Use a similar approach of the previous question but don't create any dataframe neither any list with chunks; ***Hint: Use the average definition***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-57fd7700885246eb",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12569.800000000001\n",
      "151\n",
      "83.24370860927154\n"
     ]
    }
   ],
   "source": [
    "# Read file data/exercises/world_percentage_of_literacy.tsv\n",
    "# the final average should be in the variable final_avg\n",
    "# You should increment 2 variables in each chunk and use them at the end to calculate final_avg. call them lit_a, lit_b\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "def adjst_literacy(data):\n",
    "    data['Literacy rate (all)'] = data['Literacy rate (all)'].str.split('%').str.get(0).astype(float)\n",
    "    return data\n",
    "    \n",
    "\n",
    "chunks_iter = pd.read_csv(\n",
    "    'data/exercises/world_percentage_of_literacy.tsv',\n",
    "    sep='\\t',\n",
    "    na_values = {'Literacy rate (all)': 'not reported by UNESCO 2015'},\n",
    "    usecols = ['Country', 'Literacy rate (all)'],\n",
    "    chunksize=10\n",
    ")\n",
    "\n",
    "lit_a = 0\n",
    "lit_b = 0\n",
    "\n",
    "for data_chunk in chunks_iter:\n",
    "    data_chunk_adj = adjst_literacy(data_chunk)\n",
    "    lit_a = lit_a + data_chunk_adj['Literacy rate (all)'].sum()\n",
    "    lit_b = lit_b + data_chunk_adj['Literacy rate (all)'].count()\n",
    "\n",
    "print(lit_a)\n",
    "print(lit_b)\n",
    "\n",
    "final_avg = lit_a/lit_b\n",
    "print(final_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0edcf1260d7886c9",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert math.isclose(83.24370860927154, final_avg, rel_tol=1e-1), \"final_avg is wrong\"\n",
    "assert lit_b==151 or lit_a==151, \"lit_a or lit_b is wrong\"\n",
    "assert int(lit_b) == 12569 or int(lit_a)==12569, \"lit_a or lit_b is wrong\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
