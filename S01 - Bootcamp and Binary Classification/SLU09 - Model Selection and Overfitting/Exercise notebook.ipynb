{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-05e844b835aa5580",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# SLU09: Model Selection & Overfitting -- Exercises\n",
    "---\n",
    "\n",
    "*Exercises are graded unless otherwise indicated.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-37bf85a5bd1cad30",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Data\n",
    "For this exercise we will use the Boston House Prices dataset. Sklearn has it natively. For more information see: https://scikit-learn.org/stable/datasets/index.html#boston-house-prices-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "boston_data = load_boston()\n",
    "df_boston = pd.DataFrame(boston_data.data,columns=boston_data.feature_names)\n",
    "df_boston['target'] = pd.Series(boston_data.target)\n",
    "df_boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_boston.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8f08f6d01a41b642",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Because this is just an exercise, we are going to assume we only have one feature to work with, **LSTAT** (% lower status of the population), and use it to try to predict **target** (median value of owner-occupied homes in thousands)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_boston[['LSTAT']]\n",
    "y = df_boston['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5fa6d81872f2011d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's quickly plot the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X, y, c='orange', s=5)\n",
    "plt.xlabel('LSTAT')\n",
    "plt.ylabel('target')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-af6e4ef9cfe15659",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 1: Train/validation/test split\n",
    "Let's first split our data into train, validation, and test sets. Assume the validation and test sets are each **20%** of the full dataset. Use a **random state** of 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8a287c4f6e7371cf",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# don't forget to import the function you need! \n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "# Don't forget to set random state to 42\n",
    "# You should create variables named X_train, X_val, X_test, y_train, y_val, y_test\n",
    "# There are several correct ways to do this, but to get the same answer as the autograder,\n",
    "# follow the method that is in the learning notebook\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5e6c4cd65290578b",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that the solution is correct.\"\"\"\n",
    "\n",
    "assert X_train.shape == (303, 1)\n",
    "assert X_val.shape == (101, 1)\n",
    "assert X_test.shape == (102, 1)\n",
    "assert y_train.shape == (303,)\n",
    "assert y_val.shape == (101,)\n",
    "assert y_test.shape == (102,)\n",
    "assert list(X_train.index[:10]) == [304, 83, 248, 165, 163, 199, 231, 74, 311, 455]\n",
    "assert list(X_val.index[:10]) == [375, 332, 441, 5, 154, 218, 229, 181, 23, 26]\n",
    "assert list(X_test.index[:10]) == [473, 500, 434, 381, 395, 390, 211, 175, 92, 144]\n",
    "np.testing.assert_almost_equal(X_val.loc[272]['LSTAT'], 7.73, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-31e07baf7496d077",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 2: Train a baseline model\n",
    "Let's train a baseline Linear Regression model on the train set and measure R2 and MSE on the validation set. Make sure you understand why we are only evaluating on the validation set and not the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "lr_baseline = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4908ced362205fff",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Fit the model to the training set and return MSE and r2 on the validation set\n",
    "# mse_baseline_val = ...\n",
    "# r2_baseline_val = \n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-03805e2b54b3b559",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that the solution is correct.\"\"\"\n",
    "\n",
    "np.testing.assert_almost_equal(mse_baseline_val, 33.4255, 2)\n",
    "np.testing.assert_almost_equal(r2_baseline_val, 0.49178, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6ca2dd1e6964436b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's also check the same metrics on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c407d701b2860312",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Use the fitted model to predict on the train set\n",
    "# Return MSE and r2 on the train set\n",
    "# mse_baseline_train = ...\n",
    "# r2_baseline_train = \n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1392aee655dc9a2f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that the solution is correct.\"\"\"\n",
    "\n",
    "np.testing.assert_almost_equal(mse_baseline_train, 38.8622, 2)\n",
    "np.testing.assert_almost_equal(r2_baseline_train, 0.5645, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b73a0db9866948ec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's visualize the fitted regression as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X_train, y_train, c='orange', s=5)\n",
    "plt.plot(X_train, lr_baseline.predict(X_train))\n",
    "plt.xlabel('LSTAT')\n",
    "plt.ylabel('target')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2A\n",
    "Judging by the r2 and MSE of the train and validation sets, and the plot of the model's predictions, is this model more likely to be A) underfit or B) overfit? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0c8c6dcff0cf6559",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# answer = 'A' or 'B'\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-dda35141c5402326",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert base64.b64encode(answer.encode()) == b'QQ=='"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e5074f939557f02d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "It definitely seems like this model has too much bias. It looks like the relationship between LSTAT and target is more of a curve, so let's try to make the model more flexible by creating polynomial features from LSTAT to hopefully better predict median home price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-85c6970053747fb3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 3: Train a more complex model\n",
    "Now train a Linear Regression model on the new training data with polynomial features and measure R2 and MSE on the train and validation sets. Don't forget that the validation set will need the same preprocessing that the training data had."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import expand_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "X_train_poly = expand_dataset(X_train, 10, feature_name='LSTAT')\n",
    "X_train_poly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "lr_poly = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9f9efb40ce2c90f9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Fit the model to the train set with polynomial features\n",
    "# Perform the same preprocessing on the validation set as the train set\n",
    "# Return MSE and r2 on the train and validation sets\n",
    "# mse_poly_train = ...\n",
    "# r2_poly_train = \n",
    "# mse_poly_val = ...\n",
    "# r2_poly_val = \n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-317dd26f914e243e",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that the solution is correct.\"\"\"\n",
    "\n",
    "np.testing.assert_almost_equal(mse_poly_train, 26.5562, 1)\n",
    "np.testing.assert_almost_equal(r2_poly_train, 0.7024, 1)\n",
    "np.testing.assert_almost_equal(mse_poly_val, 25.9554, 1)\n",
    "np.testing.assert_almost_equal(r2_poly_val, 0.6053, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e79182176726cc98",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's visualize our new predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X_train, y_train, c='orange', s=5)\n",
    "plt.scatter(X_train, lr_poly.predict(X_train_poly))\n",
    "plt.xlabel('LSTAT')\n",
    "plt.ylabel('target')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e8074aa03fbdd6da",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Validation R-squared increased to 0.61 over the baseline of 0.49, which isn't bad, but the curve of the predictions is quite squiggly! From the chart we can tell that this model is almost definitely overfitting.  We should generally strive to make our models interpretable, and there is no logical way to explain why median home prices would depend on the percentage lower status of the population according to such a complex function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9c75cf65a2bd703e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 4: Regularization\n",
    "Let's now try to reduce the complexity of our model by using regularization. For this exercise we will only focus on Ridge (L2) regularization, but Lasso and Elastic Net will work in a similar way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.1: L2 Loss\n",
    "First manually calculate the L2 loss on the validation set of the lr_poly model we trained above.\n",
    "Recall the L2 loss function:\n",
    "\n",
    "$$J_{L_2} = \\frac{1}{N} \\sum_{n=1}^N (y_n - \\hat{y}_n)^2 + \\lambda_2 \\sum_{k=1}^K \\beta_k^2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-692eb1183bd9f57f",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def l2_loss(y, y_hat, betas, lamb):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        y : numpy array with shape (num_observations,)\n",
    "            The targets.\n",
    "        y_hat : numpy array with shape (num_observations,)\n",
    "            The predictions.\n",
    "        betas : numpy array with shape (num_features+1,)\n",
    "            The parameters of your regression model. \n",
    "            The first value is the intercept and the \n",
    "            remaining ones are the feature coefficients.\n",
    "        lamb : float\n",
    "            The strength of the L2 regularizer.\n",
    "            \n",
    "    Returns:\n",
    "        L : float\n",
    "    \"\"\"\n",
    "        \n",
    "    # Compute the mean squared error loss part of the general loss function.\n",
    "    # Hint: use can use the sklearn function here\n",
    "    # mse = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Compute the L2 part of the general loss function.\n",
    "    # l2 = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Compute the total loss by combining the parts.\n",
    "    # L = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c7a7f5cfc2871021",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that the solution is correct.\"\"\"\n",
    "\n",
    "betas = np.append(lr_poly.intercept_, lr_poly.coef_)\n",
    "\n",
    "lamb = 2\n",
    "loss = l2_loss(y_val, y_pred_poly_val, betas, lamb)\n",
    "np.testing.assert_almost_equal(loss, 742.9, 1)\n",
    "\n",
    "lamb2 = 6\n",
    "loss2 = l2_loss(y_val, y_pred_poly_val, betas, lamb2)\n",
    "np.testing.assert_almost_equal(loss2, 2176.9, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, now we'll take a look at the coefficients of the lr_poly model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(lr_poly.coef_)), [abs(coef) for coef in lr_poly.coef_], marker='o', markerfacecolor='r')\n",
    "plt.xlabel('Polynomial degree')\n",
    "plt.ylabel('Coef. magnitude')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b5858606abf5e112",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 4.2: Ridge\n",
    "Let's see what happens to them when we use Ridge Regularization. We are going to tune the hyperparameter alpha (lambda). Fit Ridge linear models to the polynomial-feature training data using alphas of [0.01, 0.1, 1]. Use normalize=True. Return the best Ridge model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-041c01ab12c152ea",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# import ... \n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Iterate through the above choices for alpha. For each:\n",
    "# instantiate an instance of sklearn's Ridge model\n",
    "# use normalize=True\n",
    "# calculate R2 of the validation set\n",
    "# save the best Ridge model in the variable ridge_best\n",
    "# also save the best r2 value\n",
    "# ridge_best = ...\n",
    "# r2_best = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-529b23f08a82851b",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that the solution is correct.\"\"\"\n",
    "\n",
    "assert ridge_best.alpha == 0.01\n",
    "np.testing.assert_almost_equal(np.sum(ridge_best.coef_), -1.8292, 1)\n",
    "np.testing.assert_almost_equal(ridge_best.score(X_train_poly, y_train), 0.6498, 2)\n",
    "np.testing.assert_almost_equal(ridge_best.score(X_val_poly, y_val), 0.5783, 2)\n",
    "np.testing.assert_almost_equal(ridge_best.score(X_val_poly, y_val), r2_best, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-31055246c1ebde4b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see in the chart below, the coefficients are smaller than in the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(ridge_best.coef_)), [abs(coef) for coef in ridge_best.coef_], marker='o', markerfacecolor='r')\n",
    "plt.xlabel('Polynomial degree')\n",
    "plt.ylabel('Coef. magnitude')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b9be95584a0e2050",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "And when we plot the predictions, the ridge ones do have a bit of a curve while the baseline prediction is a straight line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X_train, y_train, c='orange', s=5)\n",
    "plt.scatter(X_train, ridge_best.predict(X_train_poly))\n",
    "plt.plot(X_train, lr_baseline.predict(X_train), c='green')\n",
    "plt.xlabel('LSTAT')\n",
    "plt.ylabel('target')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-47b27810ed0562c0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's summarize the results of our three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = {'baseline LR': lr_baseline, 'poly LR': lr_poly, 'best_ridge': ridge_best}\n",
    "for key, clf in clfs.items():\n",
    "    if key == 'baseline LR':\n",
    "        y_train_pred_summary = clf.predict(X_train)\n",
    "        y_val_pred_summary = clf.predict(X_val)\n",
    "    else:\n",
    "        y_train_pred_summary = clf.predict(X_train_poly)\n",
    "        y_val_pred_summary = clf.predict(X_val_poly)\n",
    "    \n",
    "    train_r2 = r2_score(y_train, y_train_pred_summary)\n",
    "    val_r2 = r2_score(y_val, y_val_pred_summary)\n",
    "    \n",
    "    print(f'---\\n{key} R-squared:\\nTrain: {train_r2:.2f} | Validation: {val_r2:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.3: Evaluate on test set\n",
    "Finally, we're going to use the Ridge model to evaluate the MSE and R2 on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ec445bf1025800ab",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Predict the median house prices for the test set using the fitted best ridge model\n",
    "# y_test_pred = ...\n",
    "# Also calculate MSE and R2 for the test set\n",
    "# mse_test = ...\n",
    "# r2_test = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-28570debb21f12f7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that the solution is correct.\"\"\"\n",
    "\n",
    "np.testing.assert_almost_equal(mse_test, 33.2675, 2)\n",
    "np.testing.assert_almost_equal(r2_test, 0.6126, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c5301880f7809704",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The test R-squared is pretty similar to what we were getting on the validation set. But since the dataset is very small and we are only using one feature, there is still a lot of **irreducible error**. But hopefully from this exercise you understand more about how to properly evaluate your models and to check for overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-970ade4434f10e80",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 5: Implementing K-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-79dcb5bbe3ccc495",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As a last exercise, we are going to try to take advantage of as much data as possible and use k-fold cross validation to approximate our OSE. \n",
    "\n",
    "Use sklearn's cross_val_score on the full dataset (still using only the LSTAT feature) to compute the mean MSE and R2 on 5 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will need this\n",
    "from sklearn.metrics import make_scorer\n",
    "mse_scorer = make_scorer(mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b0ed9bb14923e9fd",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Don't forget to import the function you need!\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Initialize model, use the settings of the best ridge model from before\n",
    "# ridge_cv = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# perform 5-fold cross validation on the full dataset (X and y)\n",
    "# don't forget preprocessing\n",
    "# store the mean cross val scores as mse_cv and r2_cv\n",
    "# mse_cv = ...\n",
    "# r2_cv = ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e89d8d233b70f834",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(r2_cv, 0.4254, 2)\n",
    "np.testing.assert_almost_equal(mse_cv, 35.0995, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-09147b020f44ba3f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "With cross-validation, the mean R-squared is actually a lot lower than what we had before, suggesting that this value is a truer approximation of our performance on unseen data, and in our previous exercise we happened to be working with a \"lucky\" split of the data. This is just one reason why proper model evaluation is important!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
